{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 - Up and Running with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure Matplotlib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"tensorflow\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure: \", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Running a Graph in a Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code does not actually perform any computation, even though it looks like it does. It just creates a computation graph. \n",
    "\n",
    "To evaluate this graph, we need to open a Tensorflow session and use it to initialize the variable and evaluate `f`. A Tensorflow session takes care of placing the operation onto devices such as CPU and GPUs and running them, and it holds all the variable values.\n",
    "\n",
    "The following code creates a session, initializes the variables, and evaluates `f` then closes the session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having to repeat `sess.run()` all the time is a bit cumbersome, but fortunately there is a better way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `with` block, the session is set as the default session. Calling `x.initializer.run()` is equivalent to calling `tf.get_default_session().run(x.initializer)`, and similarly `f.eval()` is equivalent to calling `tf.get_default_session().run(f)`. \n",
    "\n",
    "This makes the code easier to read. Moreover, the session is automatically closed at the end of the block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually running the initializer for every single variable, we can use the `global_variables_initializer()` function. Note that it does not actually perform the initialization immediately, but rather creates a node in the graph that will initialize all variables when it is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() # Prepare an init node\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() # Actually initialize all the variables\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside Jupyter it is preferable to create an `InteractiveSession`. The only difference from a regular `Session` is that when an `InteractiveSession` is created it automatically sets itself as the default session, so we don't need a `with` block (but we do need to close the session manually when we are done with it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TensorFlow program is typically split into two parts: \n",
    " * The first part builds a computation graph. This is called the *construction phase*. The construction phase typically builds a computation graph representing the ML model and the computation required to train it.\n",
    " * The second part runs the graph. This is the *execution phase*. The execution phase generally runs a loop that evaluates a training step repeatedly (for example, one step per mini-batch), gradually improving the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any node we create is automatically added to the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But sometimes we may want to manage multiple independent graphs. We can do this by creating a new `Graph` and temporarily making it the default graph inside a `with` block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "    \n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reset the default graph by running `tf.reset_default_graph()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifecycle of a Node Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we evaluate a node, TensorFlow automatically determines the set of nodes that it depends on and it evaluates these nodes first. For example, consider the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, this code defines a very simple graph. Then it starts a session and runs the graph to evaluate `y`: TensorFlow automatically detects `y` depends on `w`, which depends on `x`, so it first evaluates `w`, then `x`, then `y`, and returns the value of `y`. Finally, the code runs the graph to evaluate `z`. Once again, TensorFlow detects that it must first evaluate `w` and `x`. It is important to note it will no reuse the result of the previous evaluation of `w` and `x`. In short, the code evaluates `w` and `x` twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to evaluate `y` and `z` efficiently, without evaluating `w` and `x` twice as in the previous code, we can ask TensorFlow to evaluate both `y` and `z` in just one graph run, as show in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Normal Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code manipulates 2D arrays to perform Linear Regression on the California Housing Dataset. It starts by fetching the dataset; then it adds an extra bias input feature ($x_0 = 1$) to all training instances; then it creates two TensorFlow constant nodes, `X` and `y`, to hold this data and targets, and it uses some of the matrix operations provided by TensorFlow to define `theta`.  But the functions as usually do not perform any computations immediately; instead, they create nodes in the graph that will perform them when graph is run. \n",
    "\n",
    "We will use the Normal Equation to compute `theta`, \n",
    "\n",
    "$$ \\hat{\\theta} = \\left( \\mathbf{X}^T . \\mathbf{X} \\right)^{-1} . \\mathbf{X}^T . \\mathbf{y} $$\n",
    "\n",
    "Finally, the code creates a session and evaluates `theta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.7226696e+01],\n",
       "       [ 4.3558651e-01],\n",
       "       [ 9.3845110e-03],\n",
       "       [-1.0583513e-01],\n",
       "       [ 6.3847488e-01],\n",
       "       [-4.1102558e-06],\n",
       "       [-3.7780956e-03],\n",
       "       [-4.2437303e-01],\n",
       "       [-4.3785861e-01]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with pure NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will do this by manually computing the gradients, then we will use TensorFlow's ***autodiff*** feature to let TensorFlow compute the gradients automatically, and finally we will use a couple of TensorFlow's out-of-the-box optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Computing the Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code should be fairly self-explanatory, except for a few new elements:\n",
    "  * The `random_uniform()` function creates a node in the graph that will generate a tensor containing random values, given its shape and value range, much like NumPy's `rand()` function.\n",
    "  * The `assign()` function creates a node that will assign a new value to a variable. In this case, it implements the Batch Gradient Descent step $ \\theta^{(next-step)} = \\theta - \\eta \\nabla_\\theta \\mathbf{MSE} \\left( \\theta \\right) $.\n",
    "  * The main loop executes the training step over and over again (`n_epochs` times), and every 100 iterations it prints out the current Mean Squared Error (`mse`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent requires scaling the feature vectors first. Let's use Scikit-Learn to compute this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00  6.60969987e-17  5.50808322e-18  6.60969987e-17\n",
      " -1.06030602e-16 -1.10161664e-17  3.44255201e-18 -1.07958431e-15\n",
      " -8.52651283e-15]\n",
      "[ 0.38915536  0.36424355  0.5116157  ... -0.06612179 -0.06360587\n",
      "  0.01359031]\n",
      "0.11111111111111005\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  MSE =  3.0641024\n",
      "Epoch  100  MSE =  0.7066215\n",
      "Epoch  200  MSE =  0.63266486\n",
      "Epoch  300  MSE =  0.6016963\n",
      "Epoch  400  MSE =  0.58009356\n",
      "Epoch  500  MSE =  0.5645483\n",
      "Epoch  600  MSE =  0.55333775\n",
      "Epoch  700  MSE =  0.54525137\n",
      "Epoch  800  MSE =  0.5394185\n",
      "Epoch  900  MSE =  0.5352112\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch \", epoch, \" MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685525 ],\n",
       "       [ 0.7777808 ],\n",
       "       [ 0.14401776],\n",
       "       [-0.10215099],\n",
       "       [ 0.14182207],\n",
       "       [ 0.00536168],\n",
       "       [-0.04035838],\n",
       "       [-0.7643866 ],\n",
       "       [-0.725843  ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding code works fine in case of Linear Regression, it is reasonably easy in this case but if we had to do this with deep neural networks we would get quite a headache: it would be tedious and error-prone. We could use ***symbolic differentiation*** to automatically find the equations for the partial derivatives, but the resulting code would not necessarily be very efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets worse when a function is defined by some arbitrary code. How can we find the partial derivatives of the following function with regards to `a` and `b`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(a, b):\n",
    "    z = 0\n",
    "    for i in range(100):\n",
    "        z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21253923284754914"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func(0.2, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "a = tf.Variable(0.2, name=\"a\")\n",
    "b = tf.Variable(0.3, name=\"b\")\n",
    "z = tf.Variable(0.0, name=\"z0\")\n",
    "\n",
    "for i in range(100):\n",
    "    z = a * tf.cos(z + i) + z * tf.sin(b - i)\n",
    "\n",
    "grads =  tf.gradients(z, [a, b])\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the function at $ a = 0.2 $ and $ b = 0.3 $, and the partial derivatives at that point with regards to $a$ and with regards to $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.21253741\n",
      "[-1.1388494, 0.19671395]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(z.eval())\n",
    "    print(sess.run(grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow's autodiff feature can automatically and efficiently compute the gradients. Simply replace the `gradients = ...`  line in the Gradient Descent code in the previous section with the following line, and the code will continue to work just line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = tf.gradients(mse, [theta])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gradients()` function takes an op and a list of variables, and it creates a list of ops (one per variable) to compute the gradients of the op with regards to each variable. So the `gradients` node will compute the gradient vector of the MSE with regards to `theta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  MSE =  9.16154\n",
      "Epoch:  100  MSE =  0.7145003\n",
      "Epoch:  200  MSE =  0.56670487\n",
      "Epoch:  300  MSE =  0.55557173\n",
      "Epoch:  400  MSE =  0.5488112\n",
      "Epoch:  500  MSE =  0.54363626\n",
      "Epoch:  600  MSE =  0.53962904\n",
      "Epoch:  700  MSE =  0.5365092\n",
      "Epoch:  800  MSE =  0.53406775\n",
      "Epoch:  900  MSE =  0.5321473\n",
      "Best theta: \n",
      "[[ 2.0685525 ]\n",
      " [ 0.8874027 ]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.6614528 ]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch: \", epoch, \" MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta: \")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides a number of optimizers out of the box, including ***Gradient Descent Optimizer***. We can simply replace the preceding `gradients = ...` and `training_op = ...` line with the following code, and once again everything will just work fine:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GradientDescentOptimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  MSE =  9.16154\n",
      "Epoch  100  MSE =  0.7145003\n",
      "Epoch  200  MSE =  0.56670487\n",
      "Epoch  300  MSE =  0.55557173\n",
      "Epoch  400  MSE =  0.5488112\n",
      "Epoch  500  MSE =  0.54363626\n",
      "Epoch  600  MSE =  0.53962904\n",
      "Epoch  700  MSE =  0.5365092\n",
      "Epoch  800  MSE =  0.53406775\n",
      "Epoch  900  MSE =  0.5321473\n",
      "Best theta: \n",
      "[[ 2.0685525 ]\n",
      " [ 0.8874027 ]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.6614528 ]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch \", epoch, \" MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta: \")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also a momentum optimizer which often converges much faster than Gradient Descent by defining the optimizer like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MomentumOptimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta: \n",
      "[[ 2.068558  ]\n",
      " [ 0.8296286 ]\n",
      " [ 0.11875337]\n",
      " [-0.26554456]\n",
      " [ 0.3057109 ]\n",
      " [-0.00450251]\n",
      " [-0.03932662]\n",
      " [-0.89986444]\n",
      " [-0.87052065]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print('Best theta: ')\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Data to the Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to modify the previous code to implement Mini-batch Gradient Descent. For this, we need a way to replace `X` and `y` at every iteration with the next mini-batch. The simplest way to do this is to use placeholder nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The placeholder nodes are special because they don't actually perform any computation, they just output the data you tell them to output at runtime. They are typically used to pass the training data to TensorFlow during training. If you don't specify a value at runtime for a placeholder, you get an exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a placeholder node, we call the `placeholder()` function and specify the output tensor's data type. Optionally, we can also specify its shape, if you want to enforce it. if we specify `None` for a dimension, it means \"any size\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the following code creates a placeholder node `A`, and also a node `B = A + 5`. When we evaluate `B`, we pass a `feed_dict` to the `eval()` method that specifies the value of `A`. Note that `A` must have rank 2 (i.e. it must be two-dimensional) and there must be three columns (or else an exception is raised), but it can have any number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "\n",
    "print(B_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Mini-batch Gradient Descent, we only need to tweak the existing code slightly. First change the definition `X` and `y` in the construction phase to make them placeholder nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learining_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define the batch size and compute the total number of batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_batches= int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in the execution phase, fetch the mini-batches one by one, then provide the value of `X` and `y` via the `feed_dict` parameter when evaluating a node that depends on either of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    \n",
    "    X_batch = scaled_housing_data_plus_bias[indices]\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    \n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y:y_batch})\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0714476 ],\n",
       "       [ 0.8462012 ],\n",
       "       [ 0.11558535],\n",
       "       [-0.26835832],\n",
       "       [ 0.32982782],\n",
       "       [ 0.00608358],\n",
       "       [ 0.07052915],\n",
       "       [-0.87988573],\n",
       "       [-0.8634251 ]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have trained our model, we should save its parameters to disk so we can come back to it whenever we want, use it in another program, compare it to other models, and so on. Moreover, we probably want it save checkpoints at regular intervals during training so that if our computer crashes during training we can continue from the last checkpoint rather than start over from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow makes saving and restoring a model very easy. Just create a `Saver` node at the end of the construction phase (after all variable nodes are created); then, in the execution phase, just call its `save()` method whenever we want to save the model, passing it the session and path of the checkpoint file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  MSE =  9.16154\n",
      "Epoch  100  MSE =  0.7145003\n",
      "Epoch  200  MSE =  0.56670487\n",
      "Epoch  300  MSE =  0.55557173\n",
      "Epoch  400  MSE =  0.5488112\n",
      "Epoch  500  MSE =  0.54363626\n",
      "Epoch  600  MSE =  0.53962904\n",
      "Epoch  700  MSE =  0.5365092\n",
      "Epoch  800  MSE =  0.53406775\n",
      "Epoch  900  MSE =  0.5321473\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch \", epoch, \" MSE = \", mse.eval())\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685525 ],\n",
       "       [ 0.8874027 ],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.6614528 ],\n",
       "       [-0.6375277 ]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restoring a model is just as easy: we create a `Saver` at the end of the construction phase just like before, but then at the beginning of the execution phase, instead of initializing the variables using the `init` node, we call the `restore()` method of the `Saver` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to have a saver that loads and restores `theta` with a different name, such as `\"weights\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver({\"weights\": theta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the saver also saves the graph structure itself in a second file with the extension `.meta`. We can use the function `tf.train.import_meta_graph()` to restore the graph structure. This function loads the graph into the default graph and returns a `Saver` that can then be used to restore the graph state (i.e. the variable values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "# We start with an empty graph.\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"/tmp/my_model_final.ckpt.meta\")\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can import a pretained model without having the corresponding Python code to build the graph. This is very handy when we keep tweaking and saving our model: we can load a previously saved model without having to search for the version of the code that built it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the graph and Training Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use TensorBoard, the first step is to tweak the program a bit so it writes the graph definition and some training stats - for example, the training error (MSE) - to a log directory that TensorBoard will read from. We need to use a different log directory every time we run our program, or else TensorBoard will merge stats from different runs, which will mess up the visualizations. The simplest solution for this is to include a time-stamp in the log directory name. Add the following code at the beginning of the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learining_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, add the following line at the very end of the construction phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line creates a node in the graph that will evaluate the MSE value and write it to a TensorBoard-compatible binary log string called a *summary*. The second line creates a `FileWriter` that we will use to write summaries to logfiles in the log directory. The first parameter indicates the path of the log directory. The second parameter is the graph we want to visualize. Upon creation, the `FileWriter` creates the log directory if it does not already exist, and writes the graph definition in a binary logfile called an *events file*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to update the execution phase to evaluate `mse_summary` node regularly during training (e.g. every 10 mini-batches). This will output a summary that we can then write to the events file using the `file_writer`. Here is the updated code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to close the `FileWriter` at the end of the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run this program: it will create the log directory and write an events file in this directory, containing both the graph definition and the MSE values. Open up a shell and go to the working directory, then type `tensorboard --logdir tf_logs*` to start the TensorBoard web server, listening on port 6006 (which is \"goog\" written upside down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0703337 ],\n",
       "       [ 0.8637145 ],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.8030471 ]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inside Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the graph directly within Jupyter, we will use a tool called `tfgraphviz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G.gv.pdf'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tfgraphviz as tfg\n",
    "g = tfg.board(tf.get_default_graph())\n",
    "g.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Scopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with more complex models such as neural networks, the graph can easily become cluttered with thousands of nodes. To avoid this, we can create ***name scopes*** to group related node.\n",
    "\n",
    "For example, let's modify the previous code to define the `error` and `mse` ops within a name scope called `\"loss\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learining_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta: \n",
      "[[ 2.0703337 ]\n",
      " [ 0.8637145 ]\n",
      " [ 0.12255151]\n",
      " [-0.31211874]\n",
      " [ 0.38510373]\n",
      " [ 0.00434168]\n",
      " [-0.01232954]\n",
      " [-0.83376896]\n",
      " [-0.8030471 ]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "file_writer.flush()\n",
    "file_writer.close()\n",
    "print(\"Best theta: \")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of each op defined within the scope is now prefixed with `\"loss/\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub\n"
     ]
    }
   ],
   "source": [
    "print(error.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/mse\n"
     ]
    }
   ],
   "source": [
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorBoard, the `mse` and `error` nodes now appear inside the `loss` namespace, which appears collapsed by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a_1\n",
      "param/a\n",
      "param_1/a\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "a1 = tf.Variable(0, name=\"a\")         # name == \"a\"\n",
    "a2 = tf.Variable(0, name=\"a\")         # name == \"a_1\"\n",
    "\n",
    "with tf.name_scope(\"param\"):          # name == \"param\"\n",
    "    a3 = tf.Variable(0, name=\"a\")     # name == \"param/a\"\n",
    "    \n",
    "with tf.name_scope(\"param\"):          # name == \"param_1\"\n",
    "    a4 = tf.Variable(0, name=\"a\")     # name == \"param_1/a\"\n",
    "\n",
    "for node in (a1, a2, a3, a4):\n",
    "    print(node.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to create a graph that adds the output of two **rectified linear units (ReLU)**. A ReLU computes a linear function of the inputs, and outputs the result if it is positive, and 0 otherwise, as shown in the equation below.\n",
    "\n",
    "***Equation 9-1. Rectified Linear Unit***\n",
    "$$ h_{\\mathbf{w}, b} \\left( \\mathbf{X} \\right) = max \\left( \\mathbf{X} . \\mathbf{w} + b,0 \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does the job, but it's quite repetitive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n",
    "\n",
    "relu1 = tf.maximum(z1, 0., name=\"relu1\")\n",
    "relu2 = tf.maximum(z2, 0., name=\"relu2\")\n",
    "\n",
    "output = tf.add(relu1, relu2, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such repetitive code is hard to maintain and error-prone. It would become even worse if we wanted to add a few more ReLUs. Fortunately, TensorFlow lets us stay DRY (Don't Repeat Yourself): simply create a function to build a ReLU. The following code creates five ReLUs and outputs their sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `add_n()` creates an operation that will compute the sum of a  list of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu1\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using name scopes, we can make the graph much clearer. Simply move all the content of the `relu()` function inside a name scope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_uniform(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, 0, name=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu2\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to share a variable between various components of our graph, one simple option is to create it first, then pass it as a parameter to the functions that need it. \n",
    "\n",
    "For example, suppose we want to control the ReLU threshold (currently hardcoded to 0) using a shared `threshold` variable for all ReLUs. We could just create that variable first, and then pass it to the `relu()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X, threshold):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X, threshold) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to set the shared variable as an attribute of the `relu()` function upon the first call, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            relu.threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, relu.threshold, name=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow offers another option which may lead to slightly cleaner and more modular code than the previous solutions. The idea is to use the `get_variable()` function to create the shared variable if it does not exist yet, or reuse it if it already exists. The desired behaviour (creating or reusing) is controlled by an attribute of the current `variable_scope()`. \n",
    "\n",
    "For example, the following code will create a variable named `\"relu/threshold\"` (as a scalar, since `shape=()`, and using `0.0` as the initial value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if the variable has already been created by an earlier call to `get_variable()`, this code will raise an exception. This behaviour prevents reusing variables by mistake. \n",
    "\n",
    "If we want to reuse a variable, we need to explicitly say so by setting the variable scope's `reuse` attribute to `True` (in which case we don't have to specify the shape or the initializer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\", reuse=True):\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will fetch the existing `\"relu/threshold\"` variable, or raise an exception if it does not exist or if it was not created using  `get_variable()`. Alternatively, we can set the `reuse` attribute to `True` inside the block by calling the scope's `reuse_variables()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once `reuse` is set to `True`, it cannot be set back to `False` within the block. Moreover, if we define other \n",
    "scopes inside this one, they will automatically inherit `reuse=True`. Lastly, only variables created by `get_variable()` can be reused this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the pieces we need to make the `relu()` function access the `threshold` variable without having to pass it as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\")\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "    \n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for relu_index in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu6\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first defines the `relu()` function, then creates the `relu/threshold` variable (as a scalar that will later be initialized to `0.0`) and builds five ReLUs by calling the `relu()` function. The `relu()` function reuses the `relu/threshold` variable, and creates the other ReLUs nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the `threshold` variable within the `relu()` function upon the first call, then reuses it in subsequent calls. It just calls `get_variable()`, which will create or reuse the `threshold` variable. The rest of the code calls `relu()` five times, making sure to set `reuse=False` on the first call, and `reuse=True` for the other calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\"):\n",
    "        threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                    initializer=tf.constant_initializer(0.0))\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"\", default_name=\"\") as scope:\n",
    "    first_relu = relu(X)     #  Create the shared variable\n",
    "    scope.reuse_variables()  # then reuse it\n",
    "    relus = [first_relu] + [relu(X) for i in range(4)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu8\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(), \n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu9\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting graph will be slightly different than before, since the shared variable lives within the first ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0: my_scope/x\n",
      "x1: my_scope/x_1\n",
      "x2: my_scope/x_2\n",
      "x3: my_scope/x\n",
      "x4: my_scope_1/x\n",
      "x5: my_scope/x\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"my_scope\"):\n",
    "    x0 = tf.get_variable(\"x\", shape=(), initializer=tf.constant_initializer(0.))\n",
    "    x1 = tf.Variable(0., name=\"x\")\n",
    "    x2 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"my_scope\", reuse=True):\n",
    "    x3 = tf.get_variable(\"x\")\n",
    "    x4 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):\n",
    "    x5 = tf.get_variable(\"my_scope/x\")\n",
    "\n",
    "print(\"x0:\", x0.op.name)\n",
    "print(\"x1:\", x1.op.name)\n",
    "print(\"x2:\", x2.op.name)\n",
    "print(\"x3:\", x3.op.name)\n",
    "print(\"x4:\", x4.op.name)\n",
    "print(\"x5:\", x5.op.name)\n",
    "print(x0 is x3 and x3 is x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first `variable_scope()` block first creates the shared variable `x0`, named `my_scope/x`. For all operations other than shared variables (including non-shared variables), the variable scope acts like a regular name scope, which is why the two variables `x1` and `x2` have a name with a prefix `my_scope/`. Note however that TensorFlow makes their names unique by adding an index: `my_scope/x_1` and `my_scope/x_2`.\n",
    "\n",
    "The second `variable_scope()` block reuses the shared variables in scope `my_scope`, which is why `x0 is x3`. Once again, for all operations other than shared variables it acts as a named scope, and since it's a separate block from the first one, the name of the scope is made unique by TensorFlow (`my_scope_1`) and thus the variable `x4` is named `my_scope_1/x`.\n",
    "\n",
    "The third block shows another way to get a handle on the shared variable `my_scope/x` by creating a `variable_scope()` at the root scope (whose name is an empty string), then calling `get_variable()` with the full name of the shared variable (i.e. `\"my_scope/x\"`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Do' b'you' b'want' b'some' b'caf\\xc3\\xa9?']\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "text = np.array(\"Do you want some caf?\".split())\n",
    "text_tensor = tf.constant(text)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(text_tensor.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***10. How can you set a variable to any value you want (during execution phase)?*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a variable's initial value when constructing the graph, and it will be initialized later when we run the variable's initializer during the execution phase. If we want to change that variable's value to anything we want during the execution phase, then the simplest option is to create an assignment node (during the graph construction phase) using the `tf.assign()` function, passing the variable and a placeholder as parameters. During the execution phase, we can run the assignment operation and feed the variable's new value using the placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008577466\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(tf.random_uniform(shape=(), minval=0.0, maxval=1.0))\n",
    "x_new_val = tf.placeholder(shape=(), dtype=tf.float32)\n",
    "x_assign = tf.assign(x, x_new_val)\n",
    "\n",
    "with tf.Session():\n",
    "    x.initializer.run()   # random number is sampled *now*\n",
    "    print(x.eval())       # some random number\n",
    "    x_assign.eval(feed_dict={x_new_val: 5.0})\n",
    "    print(x.eval())       # 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***12. Implement Logistic Regression with Mini-batch Gradient Descent using TensorFlow. Train it and\n",
    "evaluate it on the moons dataset. Try adding all the bells and whistles:***\n",
    " * Define the graph within a logistic_regression() function that can be reused easily.\n",
    " * Save checkpoints using a Saver at regular intervals during training, and save the final model at the end of training.\n",
    " * Restore the last checkpoint upon startup if training was interrupted.\n",
    " * Define the graph using nice scopes so the graph looks good in TensorBoard.\n",
    " * Add summaries to visualize the learning curves in TensorBoard.\n",
    " * Try tweaking some hyperparameters such as the learning rate or the mini-batch size and look at the shape of the learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the moons dataset using Scikit-Learn's `make_moons()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "m = 1000\n",
    "X_moons, y_moons = make_moons(m, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD7CAYAAABwggP9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO29e3xV1Zn//3kSziEJgRQCVWcwof0W7TQgUePU1opOaa1grUqrowaIbR1qGEf7bWsLP9qKMrQdZzqt9qcgrSCQjGNnircK1p94w0sdYwMiOmKrhrGJDgSN5gInJM/vj31Wss4+a+299jn7XLPer9d5wdlnX9Y52Xs967kTM8NisVgsFjcluR6AxWKxWPITKyAsFovFosQKCIvFYrEosQLCYrFYLEqsgLBYLBaLknG5HkCYTJ06lWfMmJHrYVgsFkvB8MILLxxk5mmqz4pKQMyYMQNtbW25HobFYrEUDETUofvMmpgsFovFoiQ0AUFEVxNRGxEdIaI7PfZrIqIXiOh9InqLiG4ionHS548T0WEi6o2/Xg1rjBaLxWIxJ0wNohPAPwLY4LNfBYBvApgK4JMA5gH4jmufq5m5Mv46McQxWiwWi8WQ0HwQzLwVAIioAcB0j/3WSm//TEStAP4mrHFYLJbiZXBwEG+99RYOHz6c66EUHGVlZZg+fToikYjxMfngpJ4LYK9r24+J6CcAXgWwkpkf1x1MREsBLAWAmpqaTI3RYrHkAW+99RYmTpyIGTNmgIhyPZyCgZnR3d2Nt956Cx/5yEeMj8upk5qIvgqgAcC/SJu/B+CjAP4SwHoADxDR/9Gdg5nXM3MDMzdMm6aM1LJYUqerCzjrLODtt3M9EguAw4cPo7q62gqHgBARqqurA2teORMQRHQhgJ8AmM/MB8V2Zn6OmT9g5iPMvAnA0wAW5GqcljHO6tXAU085/1ryAiscUiOV3y0nAoKIzgXwSwDnM/Men90ZgL0jLNmnqwvYuBEYHnb+tVqEZYwRZpjrOCIqA1AKoJSIyuTwVWm/zwJoBfBlZv4v12cfIqIviGOJqBGOj+J3YY3TYjFm9WpHOADA0JDVIiwoLS1FfX09Zs2ahYsvvhj9/f2Bz3HllVfi5ZdfBgD86Ec/Svjs05/+dCjjDIswNYjvAxgAsBzAovj/v09ENfF8BuFB/gGAKgDbpFyH7fHPInBCZQ8AOAjgHwBcyMw2F8KSXYT2EIs572Mxq0UUIK17WjHj5zNQckMJZvx8Blr3tKZ1vvLycuzatQsvvfQSotEo1q1bF/gcv/rVr/CJT3wCQLKAeOaZZ9IaX9iEJiCYeRUzk+u1ipn3x/MZ9sf3+xtmHiflOVQy8/z4ZweY+TRmnsjMH2Lm05n5/wtrjJYCJFdOYll7EFgtoqBo3dOKpQ8sRUdPBxiMjp4OLH1gadpCQnDmmWfij3/8IwDgX//1XzFr1izMmjULP//5zwEAfX19OO+88zBnzhzMmjULd999NwDg7LPPRltbG5YvX46BgQHU19ejsbERAFBZWQkA+Nu//Vts27Zt5FpXXHEFfvOb32BoaAjXXXcdTjvtNJx00km4/fbbQ/kuOmypDUt+kwknsSx0dALo2WdHtQdBLAbk2QrPomfljpXoH0w0AfUP9mPljpVpn/vo0aPYvn07Zs+ejRdeeAEbN27Ec889h9///vf45S9/ifb2djz00EP4i7/4C+zevRsvvfQSzj333IRz/OQnPxnRSFpbE4XWpZdeOiJQYrEYduzYgQULFuCOO+5AVVUVnn/+eTz//PP45S9/iTfeeCPt76PDCghL/pIpJ7EsdFQCqKsLmDTJ+Zc58dXeHs4YLBlnf8/+QNtNECv+hoYG1NTU4Otf/zqeeuopXHTRRZgwYQIqKyuxcOFC7Ny5E7Nnz8YjjzyC733ve9i5cyeqqqqMrzN//nw8+uijOHLkCLZv3465c+eivLwcDz/8MDZv3oz6+np88pOfRHd3N1577bWUv48fVkCMZfI9xj8TTmJZ6GzY4LzcAsiGthYFNVXqxFnddhPEin/Xrl34xS9+gWg0CmZW7nvCCSfghRdewOzZs7FixQrceOONxtcpKyvD2Wefjd/97ne4++67cemllwJwEt5+8YtfjIzhjTfewDnnnJPy9/HDCoixTD5OhEJo7d6d7CTesCF9YSYLnVgMGBx0/i8EkA1tLRrWzFuDikhFwraKSAXWzFsT6nXmzp2Le++9F/39/ejr68M999yDM888E52dnaioqMCiRYvwne98B3/4wx+Sjo1EIhgU96CLSy+9FBs3bsTOnTvxhS98AQDwhS98AWvXrh05Zt++fejr6wv1+yTAzEXzOvXUU9liSGcnc1mZYzgpL2fu6kr/fHPnpn+e5mbmkhLmujrmaDTRwFNSwtzUlPp15O+sepWXO+cX141GmZctC36NMH4Hi5KXX3450P4tL7Zw7c9qmVYR1/6slltebEnr+hMmTFBu/+lPf8p1dXVcV1fHP/vZz5iZ+aGHHuLZs2fznDlzuKGhgZ9//nlmZj7rrLNG/v/d736XP/7xj/Pll1+edP5YLMZTpkzhK664YmTb0NAQr1ixgmfNmsV1dXV89tln83vvvWc8ftXvB6CNNXNqzif1MF9WQASguTm9iVB1vpKS9M4jT+BE6km8qsr5t6kp+PmXLNGfV/wOpaXJQiPIZB/G72DRElRAWBIJKiCsiWksEnaMf1hmGdn8E4kAy5YBzc1ANDq67f33nf+3tPhfx+1jefBBZ9rXEYs5piaZIL4Pa56yFBlWQIxFwo7x93Mmq5zh7m0qoSWcyGLb4ODoBD80BNTXe0/Cso+lqwsQttrycud9ZydQVja6ra4u+RxBQltt5rWlyLACYiwSZox/V1fiJK7SRlTOcPc2ldCSncgq3nkHWLFCPy55Nb9iRfLk7Z7QzzpLbXwyCW21mdeWYkRneyrEl/VBSGTLWSps7m5bvrDBt7ePfi7s+SoHeX293jfg9SotVX9Ht4/F7VsoK0t2WKfjrJevp/odLKFgfRDpYX0QxY5p7kK2QliffFK98hfayKJF3iv3w4eB5cudVXpnJzB3rjpBrbpaff2hIef4T33Kee3eDZx+evJq3u1biMWStah0zEI289pSjOgkRyG+xoQGYRIlE3YIq994dNFQ7e3qVb97pS20ANV3E5rQCSeYaRR1dTwSEpuKRlJfb/a9bThrTrAaRHpYDaKYMY2SyZaz1M/uvmiR+jjVyv3aa9XfTWhCkQhQYnC77o13r3VrNYDj1BaioLMTOO44gMj5V2gtpqU08jHJ0JJxiAjf/va3R97/y7/8C1atWhX6dfKlDLgVEIWEycSfTWepLhrqlFMcU0+85r0R992X/N1kgbh3r3rS11FS4oTJCmEwdy6wffvo58uXjwqFri69s1tF0HDWfC9pUuyE+PuPHz8eW7duxcGDB/13ToN8KQNuBUShYDrxr16dXix/EHR2964uoLHRWfWbcuRI8neTI49MtAcZUWvp7beTV/tdXYCreia2bHF8GCaTiKmGJiamFSustpFLQtT2xo0bh6VLl+JnP/tZ0mcHDhzAl7/8ZZx22mk47bTT8PTTT49s//znP49TTjkF3/jGN1BbWzsiYC688EKceuqpqKurw/r16wEgv8qA62xPhfgqah+EaZSMLhrI1LaeKp2dzKefzjx+vHM9r4xlk1ckkhx5FPRFxHzMMaNjEv6YJUv0x+gytIXPYdeu5OinkhLm3buTj2ludsYgvkem/UFjgMA+iJD9cRMmTOCenh6ura3l9957j//5n/+Zr7/+emZmvuyyy3jnzp3MzNzR0cEf//jHmZn57//+7/lHP/oRMzNv376dAfCBAweYmbm7u5uZmfv7+7muro4PHjw4ch33dZmZt27dykuWLGFm5iNHjvD06dO5v7+fb7/9dl69ejUzMx8+fJhPPfVUfv3115PGn1MfBBFdTURtRHSEiO702ff/EtHbRNRDRBuIaLz02QwieoyI+onov4noc2GOsyAxjZLZti0x+SuobV2Fn4re1QWceirw+9+P5i1EIk7imciCDsrgYLImFBRmJ1dCLsi3fHmy9iCjy9AWq9DGxmRT1/AwcMkliduExsc8+j1k05k1OWWHDPjjJk2ahCVLluCWW25J2P7II4/g6quvRn19Pb70pS/h/fffxwcffICnnnpqpBrrueeei8mTJ48cc8stt2DOnDk4/fTT8T//8z++pbuzXgZcJzlSeQFYCOBCAGsB3Omx3xcAvAOgDsBkAI8D+In0+bMA/hVAOYAvA3gPwDS/6xe1BmFKkBpLppE4cnSR6hjditxPiygr048n1bwIv5eJVuLWIkxqRAGJv0lzs6MFufcRBQFtvaaUCKRBqIozpqlFiJV8d3c319bW8qpVq0Y0iOrqau7v70865qSTTkpYzU+ePJkPHDjAjz32GJ9xxhnc19fHzE4Rv8ceeyzhOu7rMjMvWrSI77vvPr7sssv4/vvvZ2bmhQsX8kMPPeQ7/rwo1genr7SXgPg3AD+S3s8D8Hb8/ycAOAJgovT5TgBX+V13zAuIoA+ELmRWnqjdKrp7cuvs1E+6coLa+PHMlZXJ5h6/8bjNNPK5xfEitDWsV0lJ8mSvErpuwSgEi1fVWNl0Zk1OgQkkIDKQvChP1Ndddx0ff/zxCSamm266aeTz9vZ2ZmZetmwZ/+QnP2Fm5t/97ncjJqZ7772Xv/jFLzIz8yuvvMLjx48fERAf+tCHOBaLKa/729/+li+88EKePn06HzlyhJmZb7/9dr7gggtGjnn11Ve5t7c3afyFIiB2A/hb6f1UAAygGsBFAF5x7f//AviF5lxLAbQBaKupqUn68mOKIA+Ee+LftWtUKMgTtVwBVTW5ednzvV7ucalsxX7lucXxqu/t9aqu9s+TaGpK9qvIQnfXrmShJedzmIzHZloHJpCAyIA/Tp6o3377bS4vLx8REAcOHOBLLrmEZ8+ezX/1V3/F3/jGN5iZ+Z133uHPfvazfPLJJ/M3v/lNPu644/jw4cN8+PBhPvfcc3n27Nn8la98JUGDyFQZ8EIREH8CcK70PhIXEDMALAbwe9f+a7zOJ15jXoMI8kC4V8V1daP9FsSE6J4Y3ZNbU5Nae4hE9J+5J1qxglat0v0m2hkzHKFmokHImonJ/lOmONcXx7q/+4kn6gWLl3nMfS6rRQSiEBPlDh8+zIODg8zM/Mwzz/CcOXNyNpZCERC7AVwiva92aRAvu/b/hU6DkF9jXkCY4rUyLy01j0DyEgCTJ/sfLwSBajyqWkmq41UmMp0AqKtzPtf5B9zj97u+6jV1auJY/ISc1SICUYgCYt++fVxfX88nnXQSNzQ08H/913/lbCyFkkm9F8Ac6f0cAO8wc3f8s48S0UTX53uzOL78wzTypavLqUUk6hKpjlEluAmGhpypywRdlFFdnVnegojCUo3nyBGnTpPf8apktblzk6OnolHntwCciDCvKrGA03dC/n51daPTenOz/rhjjhn9vzt3RfcdUk2CstFQBcHMmTPR3t6O3bt34/nnn8dpp52W6yEZE3aY6zgiKgNQCqCUiMqIaJxi180Avk5EnyCiyQC+D+BOAGDmfQB2Abg+fvxFAE4C8Jswx1pwmCb7rF4NPPecE3La2JjYD0FMJqqQ2bCIRICGhtHeCzrEhNverh6PqZACksMX/UKC29udEht+55SFyN69wIsvjk76QGIYcXOzIxSFEAL0gri01BHi6YYgj9FyHxzk3rCMkNLvplMtUnkBWAXHVCS/VgGoAdALoEba91twQl3fB7ARwHjpsxlwQl8HALwK4HMm1y9aE5Npsk9np9pvIKKPiJiPO04foRPWq7ra/5xE+u8hO77F9w3izzD5PVP5XiecoPaVqEqaM/uH6obVnnUM+TFef/11PnDgAA8PD+d6KAXF8PAwHzhwQJk8Bw8TE3ERSeOGhgZua2vL9TDCZ9ky4I47nFVwNApceSVw663q/W6/PXnVGo06K2JhMmlqAu680/n/yScDu3alN75IxLnm0JCzqv7oR0eL5slMmAD88Y/AjTc647zqquTv0dUFHH/86FgjEeDyy4G77/Y2OXn9Lm6WLQPWrjX/fjLjxzvmL0F5OVBTA7z6qnocXr9vebmj7VxzjfP9jj3WfBym90SRMTg4iLfeeguH/cyPliTKysowffp0RFwlcIjoBWZuUB6kkxyF+CpKDcI0t0GnPeheqtIQYSSniegmdzKdCJ1tavJe+arCZktL/Z3KgH/4Ymcn8yc/aeZ8LilRO+vd28aNS96nrGz0e8klOk4/PTGKicjRSkyT5rzKfYwhLcISLsh2FFOuXkUpIExzG1Sd3bxeJ56ozop2m1BMwlVVEzpRYjKdmNBKS/WZ3l5Jd34vEaEk4/5+urDVsF8lJaOmPPF38QutlYWK170gzmW711lCwgqIQsY0tyGV1f/FFydnRbtXpukUzBOrWi8/hzsXQneuqVO9Q2dVPg13iZBUwla9xtvZ6R0SLGtLJkLFtAmU7pqZLshoKUqsgBgrBNUixERTVuaYP5qawnVYCw3Ea5KUV75eQm78eG9hJZ/HnQEtnPTyd6urS02oytfxc/DL2pLJy61FyBpQkBpbFksArIAYK6TqQxBCRbdCnzDBmaS8zCTRqLotqMkkKa9804mqkjUW+XvpSoer/DDM/uVD6uudyXvChNTG6fV3cNehKikZ1fRU39ViSRMvAWEbBhUT7e2jU4iIy6+r84/5F1FPPT2jsfnitWSJk9OwYoU6AU0QiwH79iVvHxpS51zIiWdyHkA6ORqinPeGDYnfS1c6/PLLk7ft2gVs3qy/Rn29M97Vq4GBASeaqK4utfG6GR4GnnjC+b/cte4//kPduW+M5T9YcoBOchTia8xrEIJ07O1yqWvZaVxaqtcg6uu9/Qf19YlF/0xMJKlqEiaF+MRL5bdQaUFuTae9PdE8l25zJPGKRIIVILQ+B0sIwJqYxhjpJr8J04vb1CJVj0xAnjBVJhBVdJKfiSSouaysjPmUU4KF+qqiqLwEj0AWlCUl4UZFCfOVSsDLAsRiCQkrIIoFkwY/ptpDSYm+IqkIgdWVs3aj0iy8+iaoJmcVQQSdmKTdk7WYVE2iwdzjvPji5JyN9nbvcbgFX3W1uXAIGvWluidMm0BZLHGsgCgETB5sXYMf9z5hRCJdfLF6u1uL8Ar1FJOvbpL0MpGkWuE16HXk66kc2e7IIb98BrfgC6IJiWO9jolEnByLXbucpL8Pfzgx58TkHrFYJKyAKAT8HmzT2jthterUmWpkU4sYt1f4ZaptH1WCTmXOEb0s3Ml9QeoUdXY6E63fb2IqnOrqkhP0TIW2aS0nt6ASTYzGYH0mS3pYAZHv6CZ/rzh4VTkL3Tm9XrrVv0kylsnkn2rbxyCCzj1Wr2xtFV4OdhMBpctqN8nvcL+8xusVfqsSlFaLsBhgBUS+o1uFq+oXyZOgbFqQaW93TBFe9YvcE0qQiUo1bt1xGWj76DsGr1W5mzAyrHVCUyfsdeY7WTtw+xXcdZxMXlaLsBhgBUQ+o1uFy+YCr2QzIifqSJ6ATFpqBpn4VP4Rr0SxMMMv/XwzJqtzL2FnkqFsYv7zy3gWq/+LL/YOixVZ3/L1TDWcVIS8ZcxjBUQ+o1uFe63uVStOOetW/uzii83Po5tQxOQmO6h12o0uFDad3yeI0zWIxmJiIjPx/XhpeiL81rSmlZz1LRYKQUJ3MymsLUVJ1gQEgCkA7gHQB6ADwOWa/bbDaSAkXjEAe6TP34TTLEh8/rDJ9QtSQOgmNJVdnShYATgg+OTinlDcyXJdXcnVWWVTli4U1k3QkN1MmEtMTGRBnPCqsuTp5EiIhYLpOUSSoNUaLAHIpoC4C8DdACoBfAZAD4A6g+MeB/BD6f2bMOwiJ78KUkCo8LKrm/ZGkF+PPDI6GQeNk1cly/nZ/U20CC/NQIxRLrCXCXOJn7aRqhM+Wy8T34fF4kNWBASACXFN4ARp2xYAP/E5bgaAIQAfkbaNbQERVqiqeFVVjbYbFfZtvygoZnVuQEmJvwbjp0X4TWbNzc54g2Zfh42fhqHL1ZDDW8PuPRGNjvaasNVeLSGQLQFxMoAB17bvAHjA57gfAnjcte1NOP2qDwB4GMAcj+OXAmgD0FZTU5OhnzBH6JzNQctIuyd4cQ5dFJRAF1ZpUnvINJlPVe5CJ4CyPfH5aRi6XI0w+094/b5evg+rRVgMyZaAOBPA265tf+ee/BXH/RHAFa5tZwAoB1ABYAWAtwF8yG8MeadBpFP2wK+kg+lq028f0QtCNcYgZSJ0k6jqN/GazPxMNvnkdNUJEBE0kIoQ130/sViQiwQK/5LK7Gi1CIshXgIizHLfvQAmubZNAvCB7gAi+gyAYwH8p7ydmZ9m5gFm7mfmHwN4D44AKixWrwaeeiq1ssyLFnl/Xl8/WtJbh0nZ7FgM+P3v1WM8/nj1MaJ8eHl5cnlw8ZJLeMusXq0vXS1KXMvjdl9Dd95cIMqrNzePlkGPRoGzztKXLa+v9xYR7e3O9z3rLODtt53/n3oqsHevc7yzgHLOPTjo/H9oaPT/glgMeOaZzHxvy9hBJzmCvjDqg5gpbdsMDx8EgF8C2Gxw7lcAfMlvv7zSINJxGnrVN5o6dXS/sPIdxIrUZIzp2rq9zDapZl3nklRLiXghO/D9mheFdU3LmAVZjGL6dziRTBPgmIm0UUxwTEjvAfisa3tN/NgogDIA18HxRVT7XT/nAiIsp2GQHs5hOUFNwiPb2zPb2SzTWdeZIGyhJgucsjLzv6/svLZYApBNATEFwL1w8iD2I54HAcc81Ova9zI4uRLk2l4H4MX4OboB7ADQYHL9nAsIv4Qp04fXK4pJnnzC1CBMJnu/st5jkbCFmixwUmlENJb/FpaUyJqAyPUrpwLCJGEq6MPr1cGNOdn8cOKJas1D7hInCLryNSnrbUmPVCKfRDSanH1ttQhLALwEhO1JHRay81XlNBweBu6803E8mjJ3ruOEXrbMmQ46O51t27c7zsvW1sT9X31V7Rh98MHkbSonqpdjc/VqIBJx/h+Njo6JOb8cx/lMVxfwqU85L9V9oHLg+zE05PwNRM/tIL2qZWe4xaJCJzkK8ZUzDULnqJQzgYUtWU6y8spudtuiTz89sYibznl5ySXhO00z4Ygdi8hF91SaWlgJkqZ/G9tcyMLeGkTOJ/UwXzkTEDpzjapAm/BFyA+n6kGVzyknt4kJYPJk9eQwfnz4kUCFGF2Ub3R2JtbF8vNJpSMs/EyFc+fa5kKWEayAyDRBnMVuJ7bcRlM8qH626GjUW0CE7ScoxOiifMMdceZXh0qewFN56f42Yhy2uZAljhUQmUalAXitAGUnttyljMgpdLdkiX8Ei6rchnjQbeG2/ECe7FWCW6VFqCbwdASD25SpEzqitHiqmf+WgsUKiEzS3j46mesKzwV50EtL9dqB6QRhC7flB/Jkr8pncC8q5Ak8aIirSa+K447TVwKWS4vb+yUtWl5s4dqf1TKtIq79WS23vNjiuT3XeAkIcj4vDhoaGritrS27F501a7QMQjQKXHkl8P3vA5deCtx9NzB/PrBrV7BzlpSMRrNEIskRUYL6+uQIoq4u4KMfBQ4fHt1WXg68/jpw7LHBxmFJHfnvQORMwyrkv+GyZcAddzjRZOJeYgbWrvW/3rhxwNKlwK23Jo7hIx8BjhxJvKd0iHHa+yVlWve0YukDS9E/2D+yrSJSgaY5Tdi0e1PS9vXnr0fj7MZcDHUEInqBmRtUn9kw13TYtWtUOADOg71xI7BixWgNJlGvR7zq6/3PKz/IOuGwbNnoxCKHK3rVOrJkD/nvEIkkhgXLL/lvKNehisWADRucl2DHjsS6TzJHjzrH796deC+I+0clHORw5ebm0TBme7+kzModKxOEAAD0D/Zj/QvrldtX7liZzeEFxmoQ6SBrD4JIxHkYh4b0K7GuLuBjHwP6E28YLY88Apx3nrMSFMjnXrYMuP124KqrnDwGlcai0jYsmSEVLU7WHgSiEKOY3CdPBmpr9RopEXDCCcC+fcCHPwy8+65/wcb6emDbNqt1hkTJDSVgmM+pBMLw9QFzX0LGahCZoKsLePnl5O2Dg/5JS9deay4cAODii5M1iaEhYPlyJ+lqwwZnEtm40Umi81qpFgmte1ox4+czUHJDCWb8fAZa97T6H5QtUtHiVImLw8OJ53n3XeCnP3VW+yqYnWRJZuCdd/TCQa6Q295utc4Qqamqydj+ubjnrYBIFTmzWBCNAqWlo++FyUnOVO3qAv7zPxGId99NfoBjMeC3v3VKdctln8fAQy3svB09HWAwOno6sPSBpVj24DLM+PkM0A2EcTeOA91AuREeQbLUhXnQLdh1pqRzzgGefDK98bnvk6BZ9RYta+atQUWkwmjfikgF1sxbY7Sv7p7P9L1tBUSq6B4qoT0I3A/j8uV6h6WOkpJRc4OwG3d2Ar29zjYhPFQCqQjR2XnXta1DR08HAGCInb9DR08HFm1dhKk3Tc2eoHD7nby0OF3PEF0/iaEhxxwkL0SC4p78g4zX4knj7EasP389aqtqQSCUkvrvVEqlgRzUuns+0z4MKyBSpb19tDaSUNdVDmj5YVTVTwKcCBSvB142NciOcJUDOwdaRLZV3/09+5XbvWy/3QPdWVlxBUI4poV5UBbs7e1AXZ36uAceSF6ImCIaFrkbE1m0BL2/G2c34s1vvoktC7eMLFTcDPNwoOgl3T2v2x4WVkCkg3v1pxIa8kps9Wr1g330aPJ2oSmoTA1HjwItLerIlCybBnKh+ga18wr6B/vRdE9T/ggJd4FHt2D/05/CuU5FRaLPQb5+qh0Pxwip3t/iOB2m97AQTrrFT6rPgilWQKSKbvXn9dA9+6z6XKq2oWKiV5kaZEe4TF1d1k0DuVB9VXZeAhkdO8RD+aFJqMJa3VrE668DZWXpX6u/3zFtqq6v0l4sI6R6f6uOE5j6HmThlM550iFUAUFEU4joHiLqI6IOIrpcs98qIhokol7p9VHp83oieoGI+uP/GiQPZBnV6s/voRMaxvjxidtVmsCXvuTs30Zzjv4AACAASURBVN4OLFky2gc6GgWmTlWP6eWXs/6gZ1r1Van3bjtvbVUtrmq4CtFShVNXQV7En6sih44eBU45JXGxEbT8t+yvkmlpSbw3/LQXC4DU72+vz92+B50Jy0vI1FbVZiXJLmwN4lY4famPAdAIYC0RaQypuJuZK6XX6wBARFEA9wFoATAZwCYA98W35we61d+KFf4P3erV/rHpAHD//U7S0+mnO34L4diOxYC+Pif01U0kkvUHXafihqH6eqn3ws47fP0w3vzmmzij5gwEyenJtO3WF51m2NU1+jfUOaoF9fXO4kHGHRorkO9HP+3F+iZGSPX+1n1eW1WbJBx097juHiUQ3vzmm1nJwA5NQBDRBABfBvADZu5l5qcA3A9gccBTnQ1gHICfM/MRZr4FAAH4bFhjTRvd6q+lRf3QyQ/ck0+aRzFdcAHw3HPqyKj770/ePwehiSpzT7qqr1hRLdq6yFi9X7ljJQaHNVnnCjJtu/XFHTnU2TlqThL3jS66SLy2bVMHPUSjQFNTsnlq3TrgxRf98x6sb2KEVO9v0+O8TFiZXHyZEqYGcQKAIWbeJ23bDafHtIrziegQEe0lIjnzpw7Ai5y4HHxRdx4iWkpEbUTUduDAgXTGb46pX0A8dPIDd+qp5tfpUNseEYslX18kP2U5NFFl7klH9fWzuwLq1b+XRhC2AMsIqZh8dEEPIkfGLQSGh4HLL9eHaD/xhKOxWt/ECKne36bH6e7bjp4OdPR0JPnWsn3vhlZqg4jOBPAfzHystO3vADQy89mufT8B4D0A7wD4JIDfAPgWM99FRD8AUMfMl0r7twJ4jZlXeY0hK6U2urpGC/Ede+zo++7u5LIbgOM4/tOfnDIG5eXOqu7dd1O/fnk5cMklwF13JT7koribXKytAJnx8xmewgFwYsg3XbQp4WHTHVdbVYs189Zg5Y6V2N+zHzVVNVgzb03OC6QlkEppDr9jVGVgAMeX1dnpaCDieHHcjTc6hQFFYb8iuafyGZP7nUBg8Mi9HPa9m61SG70AJrm2TQLwgXtHZn6ZmTuZeYiZnwFwM4CvBD1PTnCr3+L9WWepzQBz5yauDNNJcBLnePDBos18NfENqCKR1sxbg0hJYmZ7pCQy8kDJ/oq8Eg5A8FIXXV2OJurWHmQn99y5jjA47jjH3CRCpSMRZx+3v2z58tHCgGMs8TKXmGReC+GQi3s3TAGxD8A4IpopbZsDQLGMSYKBEV1qL4CTiEjWrU4yPE9mcUcp7d6d/F527qmcgX19ozHpst1ZhjxCNmMxYPr0os18nVI+xWg/lS+CXL+b+33eErTUhYiYcydKCif38uXOfcfsvN+yJfEeVG1racmbxMtiwDS5zm2K0pGroIrQBAQz9wHYCuBGIppARGcAuADAFve+RHQBEU0mh78GcA2cyCUAeBzAEIBriGg8EV0d3/5oWGNNGbeduLEx+b1bu/BzBqpsyDqznzvPociiTVr3tOKDmLmiKD80K3esRGwocZKNDcVyH85qQpBSF2LRATimoV27gE9+0tEKRPh0S0vifaWKalLdl7rEyyeeKKr7LBVMJ/zWPa2YetNULNq6KCEyafHWxdraYLKGW1tVqzxvroIqwg5zXQagHMD/ArgLQDMz7yWiM4moV9rvUgB/hGM22gzgn5h5EwAwcwzAhQCWwPFTfA3AhfHtuUOlDezdm/xe1iY2b/ZeGT77rL7fg5toFGhoSHxQiyzaRDXJeyE/NLkqRZB1VIuU554D/vCHxKKNpveVDrlXxNy5RXWfBcU0m1rs1z3QnXQOkQntVxssE1GB6WD7QZiiqtevIxoFZs4EXnnF6dEgOszdcgtwzTWJDm63o9GL6mrHwS3O6XYyFnjt/qC19KvLq3Fo4BBqqmrQG+tVPpjCdlsUBL1f/CgtdRzSXh0L5V4RRXKfBcUrAEK+t0wczjIEwlUNV+G2825L2N66pzWrQRW2H0QY+CUtybi1CdFhzssEJa/YVL6JsjLHfyGfs8gyYYOq0d0D3SMrOpVwyMtw1nRIJbPaC7emIfeJUPWKKJL7LCheoaiyFhBUW2Uw1rWt8zQ55TqowgoIU2Q7sTyBl5cnRom4GRoaLaznNkHpsllVE4Gc++CVlFfArJm3xrhchh/V5dV50e83VIIsUlLBLQBM6kWNAbwWLrKpyTTAQobBKfnJslVB2QqIVHCvqn77W/2Dq+sRITu45e2rV/t3F/NKyitgGmc3JoWqpkpltLK4hAOQuEhpblbXXDLlhBOSt7kjp2ynOQDAx6Z8TPuZqBBMN5BSizUhqOaRzQrKVkAERbWq6u8fVc11NfxlYjGnsJ7Oge2OatF1F1MdmyekssJp3dOKvsE+z31qq2pRXV7te66Ono78a0UaJs8+m565KRLxj5yynebQuqcVj77hHUCp6/kAOAuV5oZmlJB+qhUaiukzk80KylZABMVrVaXrU60iEhn1ObgfUHf4qs60IJq/5FkeRKorHL8bXPgULqm7xGgc2WrLmBNUobG6XtUqTCr/2k5zWLljZaDACTe9sV788g+/xDCrhbm4p4M8M9mM2LMCwg+TyVqsqlR9qnX4JUK5GxEV0IOa6grH7wbvH+zHtduvxbbXthmPJS9Ke4eFKu9F3qbrN+KGaLQDopzcWWR5NWEQxqR7dPiocrvsJwvyzGSziJ8VEG7cD4mua5zspN61C5g0CdixQ++LuOKKYIlQBVwsTRfq5xcCaHKDdw90BwolBIooF0KV9yJva2830yJEaPs77zh1vXbuHC2/MYbzHVRkMkFt4OjAyP+DaAXZzJWwAsKN/MB5dY0TZqajR4FPf9p5yLxKO2zZYjbZF0FYoVejdi9M6tKkQs5Le4eB6l7UlX4Jwr59oyU5RLRdgS5MMoGue2FzQ7ORL8wLWUMIohWEXUHZCysgZNwPnCrXwO2kHhx0nNTMwKuv6s8tH69T44skrFDntPNy5gGJN35YFE0uhGrhoMqqVpVuMUUcW6ALk0ygmoy3LNyC2867zdgX5oXQEIJqBdnKlbCZ1DJytnQ06jwo8gOnK7VtSn098KlPAbff7mRDy2WURYXOgwcTk5cKsOSyaeapH1Nvmppy6CCBUFNVgwUzF2Dba9vyt9S3CaoMamHiNMmqrq93zE9NTU75FxNE1jRzYnl7C4DRQAxdS1BT5NL12c6gFnhlUlsBITApYxCNAhMnOr0fTCBybMJicpev4S5bsGyZU4tfhXjACwTVw1MRqQisBrfuacVX7/1qoE5xwKggCmscOUdV5kU4mmXkbaqyGFOmmPciEQsTZvWCZowTtKyGF7m+J22pDRNMyhjEYsDxx5vlJQDOw/XEE+prqHoEA8nlDvI4WklHmDbSVEp2C7U8m/HiGUUVOada2MnbVGaiIIl1ooprkICJMRQFFWbgQz7fk1ZACExzDbZtS/QT+HHWWc6/Xv4Ft+BYvrzgHzQTG6kqMUiUS6YbCIu2LgpU3RUAJkQmjFyraCq8evWvFguKJUsSj3H7r7q6nFpegHOsKA2u4sQT1c2u/PwSBVxdOGhip1fgQ3V5NSZEJgS6fr7ek1ZACExzDYIWTBO5Dqrjjh4dbe7ibt6yc2dBPmimqBKDvnrvV3HFvVek7HcAkKAx5EPT94ygWlC0KiY0d+8RuVOcVznwceOCB0wUcHh2KomdurphkZIIbp5/M3r/n17w9ezZBEgmX+9JKyCCYlIwTTT26ex08iNEEpP7uMFBdXP5oSHn+AJ70IKgMv8MDg9qk4pkaqtqjRqrLJi5IOdN30NHNXG7GwQJRDKm+xi5rpeKl19OjOATeGkRBRyenYopsnF2IyZGJyZtHxweTDjOZOKPlkZTviczXbQvVAFBRFOI6B4i6iOiDiK6XLPfdUT0EhF9QERvENF1rs/fJKIBIuqNvx4Oc5xpIWsa9fXqfUQZA1nl3rZN3Y966lS9wCmwBy0IqarUYoJXreDkB611Tys27d6UUCaBQGia01RYDmo3ulIvKkRwg5/WK3wT4t9IJFjf8wIPz07VFHlo4JDvcQtmLvC9/sToxJTuyWwU7Qtbg7gVQAzAMQAaAawlIlX1OoLTMW4ygHMBXE1El7r2OZ+ZK+Ovc0IeZzhs2+bYaZuaEh2ARKOmI6FyX3ut/kFW2ZaBgnvQgpCKSl1bVYumOU1YuWOl0j8RG4rh6f1PA1CvChkcqExHXmLqK5PNo35arxAesglK9E7v7HTu8V27nH+3b08+vsCrvqZqivQ7rnVPK371h1/5Xl8naPzIRhBGaAKCiCYA+DKAHzBzLzM/BeB+AIvd+zLzTcz8B2Y+ysyvwulHfUZYY8kaq1c7vgKRgSoYHk7cNjQE3H+/+hyvvqrO0BYU0IMWhCBZ05GSCFoWtmDNvDXYtHuTZ3ihaMBSNA5qN6nU5RLHdHYCxx3nLGCOOcbbUS0n46maXckUeNXXoElqIpBCdR/Kx63csdIoRFsnaPzMR9m4x8PUIE4AMMTM+6RtuwF41r8mJ47xTAB7XR+1EtEBInqYiOZ4HL+UiNqIqO3AgQOpjj04Qq1mVmsGQ0OJKrfXCk48dAX+oAVBhML6lSuoLq/Gxgs3aguauRENWIrWQZ0Oy5ePhlC/8w5w5Ih+X3eYq9zsyq3RFlgxSTdBwrJFbo4qkMLdpMpkoo6URJSCyMR8lI17PEwBUQmgx7WtB0CyJyeRVfFxyEVkGgHMAFAL4DEAvyOiD6kOZub1zNzAzA3Tpk1LYdgpEjSaKRJxekqrEAJAXunNnTv6MBfIgxaUxtmNqIxWKj+rraoFX884+N2DIw+caWJSR08HOno6is9BnQ6i1pIfcutbOcxVUKQarWnpCi+twN2kyqTDnHBqu7UDE/NRNor2hSkgegFMcm2bBOAD3QFEdDUcX8R5zDyynGHmp5l5gJn7mfnHAN6Do2XkB26nnAkiyc40lLZA48mDYqomL3twWeBzM0bDDDNZ0KwgWL7cbEGji3ySPy9Sv5gJXlqB/Fnrnla8f+R9o3OKEG+T/tby9mwU7QtTQOwDMI6IZkrb5iDZdAQAIKKvAVgOYB4zv+VzbgYMA4qzgYn2EIkk2nnLy9UOPjcFHE+eCqZq8voX1qd0fgaPlN4Ys8Jh1y6zGkzC0e0X+VSkWoQJXuYb+TNT/4NgcHgQ33jgG77XcW/PdNG+0AQEM/cB2ArgRiKaQERnALgAwBb3vkTUCOBHAD7PzK+7PqshojOIKEpEZfEQ2KkAng5rrGljkgsxOOjYeUVCkulDVcDx5Klgoia37mn1rQTrRcE7ptNl0SLvz0WvEr92o4JYDNi0qegXLyrWzFuj7JvuzmVI5Z7rG+wb0SKy2fPBi7DDXJcBKAfwvwDuAtDMzHuJ6Ewi6pX2+0cA1QCel3Id1sU/mwhgLYB3AfwZThjsfGZOPb02bHS+AjlcVeQ8yKGDfhpBgceTp4KfmiycdV6UUAlaFrZoyxuY2IKLFpM2uPfdl3zMpEnJ93VzsxPOXVcHDAwU/eJFRePsRmy8cGNCcEV1eTU2XLAhYfWeqqNY+Biy2fPBC1vNNR2WLVNXutSVVSZyPtM1dVFV7SzAct9+BClrbFo1swQlGIbaJFJdXo2D3z2Y1pgLFnGPeplEp04F5AhA1X2tqnasqhhbhLjvV5MS8qmWAycQhq8PEPwSAraaaybQ+Qq6utR1cQBnFfbAA/pzjoEw1yDZn617Wo0jl3TCAXDalGayHEHeIt+jXvT1jfamFl3pvLooCsaACVR1v65tW+t7/zbObkTTnKbA18u3MGwrIFJF5ytYvlydFyFMTv39+gbxBR5PboJp9mfrnlYs2eqqUJoG8gO9eOti0A1UWMIilVLaq1ebdZgT3ehEQpxfF0XBGDCBmuTe6O7fTbs3BbpWPoZhWwGRCl6+ggcfVB/jbuc4hkJZZXQagdguskcXbV3kqRWkg6jPlInaNRnD637RCY9nn/Wu2iqIxUYT4fbuTb6vr71Wn1RX5FqEqbN5f8/+hMznxVsXBzYv5WMYthUQqeClbh9/vPexsZjjZ1i7dsyEssqUkqJgYXy7rM5ni3xu1jKCX+izTnjoCkQKRC8J4XxWIcrE6HyVRWYCdWNq8plSPiXBFCUXiTShlErzTjgAVkCkhpevQJiJmpv1D6e8GivyFZiMV7jqEA8ZqfOZwGuVmOlyykZ4hT57CQ8/E5PoJbFhg95PIZeJKS8fLdonRzgVkQnUjUnNsGhpFIcGDqV176YTxp1JrIBIBdlXIFZfy5aNPijioTWx/44BOy7gH65aW1Wbs3wF1SpRFGRbtHVRRssp++IX+mzSxlZHLOb0I1GZoSoqkjvVyb6KMbKoUYWbNjc0j7yvLq8Gc3CNwY2uv0musQLCjc6eq9quW72tXq23/U5UlKYqkjajXnhpB8I5l4sIDpVjUAgzVUG2rJukvMyZpm1s3RCNJscde6x6v/5+YOlSJ1xbmJdkX8UYWNQI3NnKt51328j7ymhloIxpFfnonBZYAeFGZ89Vbdet3p58Uq89fKAoTSVWckW8MvPSDoRzTqXOm7ZsNCVaGkV1ebVn8pGfqSurmo6XOdNLeHhlQsvh1nPn6v0PXiHZY8A0qjMvytuD+suEpiB8cfleI8wmysnIyUByEpBqO7M+ceh73zOrfQM4iXCXXQbcfXfydYsIXcKbqJMkUCXRrdyxMiXHdSmVKm27folzJTeUeJoM3GPOGSef7PgE3IhOcoBj+ly71hECsjApKwPeeAOYP199DhMK7F71S9CUP6+IVKBvsC/h+IpIBZrmNGHT7k0p+Rvy5r5xYRPlTNFpBKrtutWbroG8jljM2b/I6y+Z1pZRFR8L0lxIPrfO8dc90O3pR/AydeWVOcAvb6ary3FAA8n3aizm3GfyOdwdDf0ooHtVlfC2eOvikSrB7s/dwgFwzIvrX1jvKxxKUJLUDjev7psAWAEh0NlzRWape/uTT6pV/9/+Vm9emjw58WGcOxe4+GLg6NGir7+Ubm2Z8nHlI/+vjFYmFUyLlEQSTEdNc5q0IbUAPP0Iqn7XQHJDmLzHyxc2POwID3fUU5AeJwUU4qprQSs6EJpG0JlEG00un4wNF2zwvdfzIkLOh3G5HkDeoNMI5MxSeftZZwEvvZS4XZiidLz/vvNAHnuso2k8+aR6P7EyK6L6S4AjJHSTq079V9W0GeZhXHnKldp6OOIYr4fZz4/gNr1GSiK4ef7NhSMcTMpsCC1C3GcmVYqBgqwPpvt7iw6EYfqVDg0cUt7r8j0+pXwKPoh9MNJXXUTIAcire8xqEIDzMG3erNYI/vQn8/pIfiswOfrEq7tXAa3MwsCrPpOuNMe217Zp6+CbrAZrqmq0KzhVLX/R+Svv0EXdmWgDw8PAjh2j74W5qb7e+7gC1HK9zIZikZHJa7nv8e6B7hHhIMjHpE0rIADnYRoYGG21KL8GBszrI5mswJ54Qt/dS2S2FnnykRuv+kxenbV0E7zfapBA+NiUjyUJpUVbF6HyR5Vah3he9pXQRd2ZagPjFEYEISi87sEC8j8AjtlQFxEnNNCgfi4VOl+DqQkr3+4xKyDC7ODW3p7YC8JNNAo0NOi1hwJ76MLCSwjoVnbu0gay1uG3GmQwHn3jUeUDq3JOCvKt0qZnRWG5n0Nnp5P7oOLll/X3vFejoQLTchtnN+Kqhqu0fcrdPrJUwqu9/GqmUXj5do+FKiCIaAoR3UNEfUTUQUSXa/YjIvonIuqOv24iGr2DiaieiF4gov74vz46bxqE3cFNV80VcB6q++/3LmtQQA9dWHi1V9RFPwFQah2Lti5Cb6xX6WSW8ct81U0keYVXReEnn3T+FftF4k79aNRp+BON/z6RiL4AoK7RUHV1QWq5t513G7Ys3GIUKDGlfIqyc5wOv7a2JgInH++xsDWIWwHEABwDoBHAWiKqU+y3FMCFcHpWnwTgiwC+AQBEFAVwH4AWAJMBbAJwX3x7uJh0cAtaZllXzVX0+/UbT4E9dGHgFQKri346NHBIe77ugW4wMyqjlSmPSfSyzmU3L0+8ou5EmHVLizoKT1WxVfSDkKsBRDQTpChZX4DoejirfARBMqQ7ejo8I5G8FiR5e48hxEQ5IpoAp03oLGbeF9+2BcCfmXm5a99nANzJzOvj778O4O+Y+XQiOgfARgDTOT44ItoPYCkzP+Q1hsCJciYd3HRd41Soum6JhCSRTDR1KtCt6Z66bFlBRYaESZAuc4B5p7lUydekphF09+5HPgK8+urothNPdO4/L39ENArMnAm88srofa5LwhP7F1gUkx9h3U8VkQrlRE836DUIvj63ycrZSpQ7AcCQEA5xdgNQaRB18c9U+9UBeJETJdeLmvOkh18Ht6D+CVXkiAglFHiVAy+wyJAw0a3sdKTrVPRT+f1WhDlHd+/KwgFw3vs5q2Mxx5wk3+deCXQFGMWkIp2SGTp0zYNKSD3Vyr2t85EwBUQlgB7Xth4Aiup0Sfv2AKiM+yGCnAdEtJSI2oio7YDcV9cEv0zUoP4J1UM7POxELsnX1Dmyx6iTOhVk01MqCBOSF3ndUEh178qVV2UuuUR9n8sViYU5SXUPFmG7UbdJKUzkoIvPbf6c0/yKk/2O0dIobp5/c6jXDpswBUQvgEmubZMAKKrTJe07CUBvXGsIch4w83pmbmDmhmnTpqU0cCUm/gk327Y52dFNTaNOwGjUse3K6Or0j1EndRgEjTqpLq820kLyMTZdi87/dd99+mNM7vMi7JWeyd4jIuhi2YPLsOONHcp9SqkUGy7YkHc+BzdhCoh9AMYR0Uxp2xwAexX77o1/ptpvL4CT5KgmOI5s1XkyRyqrptWrgZ07Heeg7oFT1ekfo/kP6bDswWVYvHXxiGkglVWg2wGuQ5ib8rkkAgC9+TIW0y9sTO7zIuyVnql8AzkSaf0L67X7DfNw3gsHIEQBwcx9ALYCuJGIJhDRGQAuALBFsftmAN8ior8kor8A8G0Ad8Y/exzAEIBriGg8EV0d3/5oWGM1IuiqSUz8zMnagZxBfeqpRaeuZ5vWPa1Y17YuLdOAiIKSfR86kxOBcts0yBR3Iyu/UFagKLUDE8LKN4iURBKi5eSaYV6lXvIt30FH2GGuywCUA/hfAHcBaGbmvUR0JhH1SvvdDuABAHsAvATgwfg2MHMMTgjsEgDvAfgagAvj27NH0FWTV2kDuX5/V9eYfCDDZOWOlWnbjUuoJEkj0PWjcF8r781OQcyjXvd50BDvAsIrszoIc2vnJpTM6B7oxtfu+xpa97R6FovMt3wHHbYfhAldXcCllzo9G1S171Xhre5a+bpeE5bA+PVrCEq0NIqJ0Yk4NHAIU8qnAHA0jJqqGs/oFgIZheRmHZPwbdPzmIZ45zmqMOqn9z+dtiaqo7q8GpfUXYK1bWuTPpv3kXl4ZMkjoV8zVWw/iHTR1buRP3drD4cPAytWqPexJqW0EJO4ipIUbunYUMxJrosnSA0cHcCWhVvw5jff9Ix0yluTUxhmozBL0OQYXTHIM2rOwJaFKgt4+nQPdOOMmjPQ3NA8okmUUimaG5rzSjj4YTUIP7xW/l1dwEUXAXv2ONmlbqqrgYMHzTQMizFTb5qq7BcdLYkChKQqmakgEuVU5ca99i8aZC2kwBPj/LoZZirpUpc0l29YDSIdvFb+q1cDzz3nCAdRCVZOKhIlCVQaxtGjwCmnFPTKLFfoymzEhmO+wsHLLiwjolxMI53yrQpnWqQS4p3HeBWDBIAFMxdk5Lp576sywAoIL7weFLmdIzDanUslUFQq/+Cgcw5ragpMOhEgJh3BAMeMJUJbV+5YiTXz1nhGOhVKVIoRRZYY51UMEgC2vbYt8DlNFxqZLAeTDayA8MLrQXG3c4zFnOqZKoGyfXtihIisZRTwyixX6Ir7hVW2IFoaxftH3leGtpr21s4LUo1CKrLQV7+/WdBJvLmhGZsu2mRU6oVA+eWfCogVEF7oHpQnnnA0Bll4DA87CXK6HAgZ67BOC12F15vn35x205cJkQmYGJ2YVMlTlBJfuWMlmuY05XelV4FfcIWOIkyMk/MT3L3FTbUBwa/3/jrpHtQtTkRL00LFOqlTQYT/mTZ4r68ffbjGsMM6aMXWdK6RSdW+IJyPNqwaAJRBBu6/n1elVR0tC1uMK7YSCMPXG84VOcA6qcPm2Wf1wkH0fdCtvIrMvmuKV9/pMBGZ0Xw9J4UYRkvMWor4JVAVhPNRvs/GcECEVztbQSoFH6/dfm3SNhP/lK5Nbr5iBUQq6FRwEzW8yOy7ppg8qGHSuqcV217bhmF2HMubLtpk3ADGJHEqr6OW3MEVIiBi+XLv4woQvwnXL4IJSK10fPdAd9K1/Hwd2VokhYkVENmmCO27Jpg8qGGhexC9EuyC4hW1lPNVoq7sS0tLXmsRQX83vwnXqw9DCZUknF/2UZjiXty4S9CXUunIIkiYPrO5SAqDcbkegGVsoCtbkUp4qJ8vQ/cg9g/2K2srBcUraslt8xaTFoDs+SxUWiowasrMw4S3VH43vwl36QNLtWHNQzyEpQ8sxdP7n8am3ZtSKv2tWtyIsaq+i+4a+ayNWg3CkhXCCg81UdO9Hrh0hYNf1JLXpJU1zUJoqe5OcEDehlUHWV2L31EXiLC/Z79Rv4f+wX6sf2F9yn0hdIsb3XfRRUvlcw6NFRCWrKALTQ26qjaZSDL5wPXGej0/1wknIciyan8uoIAIUxOkvEDQUVNVY7wqN02cdBMpiWgXN7prD/FQ4eTQxLECwpI1gvadVpEpp6Mp3QPdWLR1EegGUmoBOuEk7NEyGbc/F1BAhO53Y3DC72yiGSyYuSDjq/IrT7lSe//qri0WRQWRQxPHCghLQeFXNgFIv1+1KSotQGdK061UM2p/LqCAiDXz1iBaqg5Dln9nk99rbdvajJe48CrP4WVODWORlE2sgLAUFKa+jMbZuX2N/AAAFf1JREFUjVgzb41R+Q2vTFg/3FqAzpSmE1ZyzadCiIvPJF5Ju+J3TkczmBCZkPKxbrwEVVjm1HwgFAFBRFOI6B4i6iOiDiK63GPf64joJSL6gIjeIKLrXJ+/SUQDRNQbfz0cxhgtxYHpwyds1aqy4G4YjEvqLkl5TO7JQrVKVAk2r5pP6ZLzUNuAY1q5Y6Vvnsr+nv1pmQ/7BvtSOk6Fn6AqNE1BR1gaxK0AYgCOAdAIYC0R1Wn2JTjtRCcDOBfA1UR0qWuf85m5Mv46J6QxWooE98MHIGniMbFVy6g6f5lSU1WjnfzE9sVbF6N8XDmqy6tHBJuu5lO6fol8TMjyG5OJ6aimqiZr5kMv8t2xHCZp12IiogkA3gUwi5n3xbdtAfBnZvZN3SSiW+Lj+If4+zcBXMnMgdsuZa0WkyVv0NXaSTV0MRWaG5qTYukrIhX41PRP4dE3Hk0IrZXrAOlap6Zbu8evQU4uSLdpj6r+VaYa/fjR3NCM2867LevXzRSZrsV0AoAhIRzi7Aag0yDkgRGAMwHsdX3USkQHiOhhIprjc46lRNRGRG0HDhwIOnZLgRM05jxsqsurse21bcox7HhjR5IA6B/sR9M9TWjd02rkcE+FbGatm+I3JpXpSNTEcpsR/fIgMs2v/vCrvDDZZYMwBEQlgB7Xth4AEw2OXRUfw0ZpWyOAGQBqATwG4HdE9CHdCZh5PTM3MHPDtGnTAgzbUgwEiTnPBDfPvznwxCuyeBfMXGAcFx/Ep5ApwZMOfmNS+Za2LNwCvp4TbPgmeRBBCRqgMDg8mNflMcLEV0AQ0eNExJrXUwB6AUxyHTYJwAc+570aji/iPGY+IrYz89PMPMDM/cz8YwDvwdEyLJYkTGLOgdGa/9Xl1dpwyqCMK3Eq1aRS46l/sB/bXtsWyOFu4lNo3dOqTebrjfVmfOWrE2Q6J31vrHdkXwCh+5b8mPeReTj43YOBfRr5XB4jTHxrMTHz2V6fx30Q44hoJjO/Ft88B8lmI/mYrwFYDmAuM7/lNwTAp/6yZcyyZt4apQ9CxJyrokfknhHp1GY6OnxUWfbZFK9JRq43VUIlSXkUwlS1eOvikXpUADxr/nQPdGe0LpRJPSXxnaaUT8H7R94fiTJz76s7V6rCQfd3fvatZ0c6BQY5fz6XxwiTUBoGEdG/w5nIrwRQD2AbgE8zc5KQIKJGAD8F8DfM/IrrsxoAxwN4Ho528w8Avgvg48zsG69ondRjk3QaEbXuacW12681CocNmwmRCWBwknBrmtMUuIBcRaQC5ePKA32P2qra0Jo2te5pRdM9TcqEwFIqxTAPJ/xt/JzWU2+aqvwupVSacnkM3bHV5dWojFaio6fD6PyRkgg2XrixYENX3Xg5qcMSEFMAbADweQDdAJYz87/FPzsTwHZmroy/fwPAdABHpFO0MPNV8dDYuwD8HwCHAewC8D1mNpr1rYCwpEK6Ds90Jq1snM8Lk+54fgJYFUnmd71FWxdp92lZ2OL5eaaj1LyEbQmVYPNFm4tGOABZEBD5ghUQllTQhZsGIduhtWHiFf5q0rIzqICtrarFW++/pdU2pk+a7nm+5oZmrGtbF/hvFkTwVpdXY+DogOf3LhZsy1GLxYN07cl+5TTCopRKQSBtCG91eXVKkVtevhCT8uVBta+Ong7PPg1+DuBNuzfhqoarRoIETKiIVGDpqUuNf5/uge6EcOlCLpeRDlZAWAqedMtKpFv9dcHMBSMmmEwyxEOoqapRTnQVkQrcPP9mrD9/feCwTS8BaVK+PExqq2p9BbaIALvzwjuNz9s0pwm3nXcbmuY0+fYclxHh0mH5agoNKyAsBU0YZSXSLd9wR/sdI9fPNB09HVjXtg6fmv4pZXhs4+xGVEYrjc/nVzYiSPnydCEQOno6nNBXn6lpf89+NM5uNP6bieqr217bFvjvlO9tQTOJFRCWgiasPr+ivpPX6lL3WWxI0d4zRXQ9lGUYjB1v7MCCmQuUxeD8NJkgZpOg5cvTQUzc3QPdKCkp8ay+KgSXqfYnfpNUtbyxkvfgxgoIS0ETdlkJr8S7LQu3BC7hYTLhywyzeQ2mtW1rlY2LvL4DX884+sOjSRnKOoKWLw+Lo8NHMXB0AECyYJa1HlPtT/wmXuYrAmm1rynlUzD1pqmgGwh0A2HqTVPHRLkNKyAsBU3YZSX8mr0EmcBrq2qx+aLNoWVu63Cb1dLp/y38OXQDYdyN40A3EFbuWIk189YkaCsLZi4INMaKSEUg0xcwKixlk1B1eXWS1iO0P52QINDId/fSOBiM8aXjlRnf7x1+LyHstXugG1+996tFLySsgLAUNLoHPtWyEn79JoIIHmEn33DBhsDjCEr/YP9IVneqDWvcdY6EGUnl1/HqqOamlEqx/vz1WPfFdWkLS6FVqNBpjQxO+O7l48q15zg0cCjpt4uURJQmtbFQk8nmQVgKHl02dNC4dZOM7CBJYSKDeEr5FBwaOJQVJ3Y6paj9QlblfIkguSOifHlYWeu6vA2/7GyTv5373K17Wj2T9tItzZ4P2DwIS1Gji9wJ4qw2jYYSq3MT38IQD4HB6B7ozopwAIB1betSNnv4hazKK/QgmpRoqGTa4c+Pjp4O5Xf0M635FfpTmeH87p9ir8lkBYSlKEjXWR0kGqpxdiMml03WnitbvShUMDhls4ffuOXOeab5DxWRCiyYuQBN9zSFGhYrhLecA7Nyx0o0zWnSmta87gWdGc7rmEhJpOg7y5mnIloseUxNVY1y0jJd4QUVMIcGDim3EyiQIzsTpBrB5RW6Gi2NYsHMBYEqnlaXV+PI0JG02rnqED4XuRxGR08HNu3epDUr6u4Rr1IjumMIVFQF+3RYDcJSFKQTuQPoezrotntFT+Xa7JDK9Vv3tHpqEMyMX+/9dSAtoHugW9ubwk20JLjzWpTDkPEyK6Zyj+iO2bJwS9ELB8AKCEuRkGrkTqp4TTbplu5QUVZaZlQiws/s4S5L8rnNn0PJDSVYtHWRpwYxODyY0ZLox008Dnw9h5JfodOgUrlHsn1f5Rs2isligT4qxytKxSvqSf5MaCHZ6jmh6/MQJAIrF1SXVwf6jXT7i/4OqfQHGYvYct8Wiw9+IZJhQDdkrzGiCPEF4NmZrlCpLq/GzfNvThJ40dIomBmDw4Mj24q1THdYZCXMlYimENE9RNRHRB1EdLnHvquIaJCIeqXXR6XP64noBSLqj/9bH9Y4LRYV6fow/PALPQ078ql/sB9L7lmCr9771ZHQ3WIRDqJyrcr8MzE6MUE4AGO72F66hOmDuBVADMAxABoBrI13iNNxNzNXSq/XAYCIogDuA9ACYDKATQDui2+3WDKCqa051dLifhOUKCsdtFS3F8M8nDRZZpogPRpMaW5o1v5dRJmN4euHsWbeGq2JaqwW20uXsFqOTgDwLoBZzLwvvm0LgD8z83LF/qsAfIyZk1IUiegcABsBTOf44IhoP4ClzPyQ1zisicmSSUy6q+kwzTwuoRKUUmnWJ/YwqYxWYmBwIBSNxdTE5+dfCdNUWGxkw8R0AoAhIRzi7AbgpUGcT0SHiGgvETVL2+sAvMiJkutF3bmIaCkRtRFR24EDB1Idv8XiSzqlxU1DT4d5GETkmakdKYkYnStX9MX6cPSHRwM15lERxMTnlSUdpqlwrBGWgKgE0OPa1gNgomb/XwP4KwDTAPwdgB8S0WWpnIuZ1zNzAzM3TJs2LZWxWyxGpJOtHST0NTYUw+Syydr98127MCmt7UfQcFKvv4F1UKeOkYAgoseJiDWvpwD0ApjkOmwSgA9U52Pml5m5k5mHmPkZADcD+Er840DnsliyRTqlxd0+juryas/Kpt0D3Z5VR7NBKo5zebUeNB+ktqoWLQtbjHtVyHj1wLDCIXWMBAQzn83MpHl9BsA+AOOIaKZ02BwAew3HwcCIProXwElEJOunJwU4l8WSEdKNdBIO1S0Lt6AyWunbiS6svAm/iZ5AScKqIlKh7H3tdx23A1kWirpxVJdX+woFv+CATEehjVVCMTExcx+ArQBuJKIJRHQGgAsAbFHtT0QXENFkcvhrANfAiVwCgMcBDAG4hojGE9HV8e2PhjFWiyVVwsiqdfdcyAYmzuINF2xI+l63nXfbSC6FHxWRCmy6aFPSbyFHGelqVOnqWgl0lXaXPbjMuFCfJTVCS5QjoikANgD4PIBuAMuZ+d/in50JYDszV8bf3wXgHADjAbwF4DZmvkU618kAfgXgEwBeAfB1Zm73G4ONYrLkO7qEvFIqzVmegl+Ej0n11paFLZ6TceueVjTd06T8jqlen0AJkWE2IS41spIox8yHmPlCZp7AzDVCOMQ/2ymEQ/z9ZcxcHc9/+LgsHOKftzPzqcxczsynmAgHiyVfkc0juol2mIcz3udZBYHQ0dPhmdPh50vw640hNACVcDAxA3l1ipOxCXHhY4v1WSwZxG0e0SFqBvnZ/EWehEy0NIrmhubATmV5Ba5rkASMmtZ0SXzDPKw9FtCHoLp9FjqCtnm1hIcVEBZLBvHrYgaMrqJlH4cOZsamizYl2No3XLABt513GzZdtMnIqVxbVYvaqlrtClzlEG6c3Yib59+sFRJeq3cvrcnEHKQSnLoci1yXWi82bLE+iyWDeGVQE0hbbTTV4oF+fZ8JhKsarsK6tnXacUVLo0kRVhMiExAbinnmYKgq37buacXirYuV1wqS3eyunLtg5gJs2r0ppax2SyK2mqvFkiPSmehTLeshjl+5Y2VWo6VU38nLwZxu0x2vcusWc6yAsFhyRDoTfRgTYJD+0emg+05eGhRfXzxzTyHjJSBsT2qLJYOICTOVib5xdmPaK+JsOG11DYoA7z7QlvzHOqktlgySbTOI28Gs66kdBhWRCrQsbPHMgE43wznV8uqWcLAahMWSIdzmJRFKCiAjQkJ1vUhJROl0lnEnnEVKIhg/bjx6Y73aY0RHN7/vkY4Gle3fz5KM9UFYLBkiG21MTa4nQlNVkU3R0ii+fvLXse21bUkTeDrZz2GQ7d9vrGJ9EBZLDkinPHiY1zs0cAjD1w8nhcD6aQGNsxuxeOviQNcKk2z/fpZkrICwWDKEzkGbqWQuv+ul4vTO9nfIl2tbHKyT2mLJENkuQZ2J6+WyjLYt4Z17rICwWDJEGOXBc329bH+HfLm2xcE6qS0Wi2UMk5Vy3xaLpTiwuQcWQSgCgoimENE9RNRHRB1EdLnHvtuJqFd6xYhoj/T5m0Q0IH3+cBhjtFgs/ui6t5kKCStciouwNIhbAcQAHAOgEcBaIqpT7cjM8+ONgirjTYSeAfAfrt3Ol/Y5J6QxWiwWH1TlyU0b8aQrXCz5R9oCgogmAPgygB8wcy8zPwXgfgDqAOrEY2cAOBOa3tUWiyW7pJN7kI5wseQnYWgQJwAYYuZ90rbdAJQahIslAHYy8xuu7a1EdICIHiaiOV4nIKKlRNRGRG0HDhwINnKLxZKALsfAJPfAJrYVH2EIiEoAPa5tPQAmGhy7BMCdrm2NAGYAqAXwGIDfEdGHdCdg5vXM3MDMDdOmTTMds8ViUZBO7kE6wsWSn/gKCCJ6nIhY83oKQC+ASa7DJgH4wOe8nwFwLID/lLcz89PMPMDM/cz8YwDvwTFDWSyWDJNO7oFNbCs+fEttMPPZXp/HfRDjiGgmM78W3zwHwF6fUzcB2MrM+pKR8SEAmga0FosldFLtQ5FO5VZLfhJKohwR/TucifxKAPUAtgH4NDMrhQQRlQPoArCQmR+VttcAOB7A83C0m38A8F0AH2dmdZNdCZsoZ7FYLMHIRqLcMgDlAP4XwF0AmoVwIKIzicitJVwIx0/xmGv7RABrAbwL4M8AzgUw30Q4WCwWiyVcbKkNi8ViGcPYUhsWi8ViCYwVEBaLxWJRYgWExWKxWJQUlQ+CiA4ASG5BlX2mAjiY60EEoJDGW0hjBQprvIU0VsCONyxqmVmZZVxUAiJfIKI2ndMnHymk8RbSWIHCGm8hjRWw480G1sRksVgsFiVWQFgsFotFiRUQmWF9rgcQkEIabyGNFSis8RbSWAE73oxjfRAWi8ViUWI1CIvFYrEosQLCYrFYLEqsgLBYLBaLEisgQoCIro63PT1CRHca7P9/iehtIuohog1END4LwxTXnkJE9xBRHxF1ENHlHvuuIqJBIuqVXh/Nh/GRwz8RUXf8dRMRZb1vSIDxZv23VIzB+D7N5T0qjcFovER0BRENuX7bs7M3UoCIxhPRHfF74AMiaiei+R775/z3NcEKiHDoBPCPADb47UhEXwCwHMA8OK1VPwrghkwOzsWtAGIAjoHT3nUtEXn1D7+bmSul1+t5Mr6lcMrGzwFwEoAvAvhGhsemIsjvme3f0o3RfZoH96jA+LkC8Kzrt308s0NLYhyA/wFwFoAqAD8A8GsimuHeMY9+X1+sgAgBZt7KzPcCMOlb0QTgDmbey8zvAlgN4IpMjk8Q7/73ZQA/YOZeZn4KwP0AFmfj+n4EHF8TgJ8y81vM/GcAP0WWfkdBvv+ebgLcpzm7R2UCPlc5hZn7mHkVM7/JzMPM/FsAbwA4VbF7Xvy+JlgBkX3qAOyW3u8GcAwRVWfh2icAGGLmfa7re2kQ5xPRISLaS0TNmR1eoPGpfkev75EJgv6e2fwt0yGX92iqnExEB4loHxH9gIh82ylnEiI6Bs79oeqqWTC/rxUQ2acSTjc9gfj/xBxcW1xfd+1fA/grANMA/B2AHxLRZZkbXqDxqX7Hyiz7IYKMN9u/ZTrk8h5NhScBzALwYTga3WUArsvVYIgoAqAVwCZm/m/FLgXz+1oB4QMRPU5ErHk9lcIpewFMkt6L/3+QhbG6ry2ur7w2M7/MzJ3MPMTMzwC4GcBX0h2nB0HGp/odezm7mZ/G483Bb5kOGbtHMwEzv87Mb8RNO3sA3Igc/bZEVAJgCxy/1NWa3Qrm97UCwgdmPpuZSfP6TAqn3AvHsSqYA+CdMPpuG4x1H4BxRDTTdX2VGqy8BIBMrtCDjE/1O5p+j7BI5/fM9G+ZDhm7R7NETn7buPZ6B5yAhS8z86Bm14L5fa2ACAEiGkdEZQBKAZQSUZmHDXQzgK8T0SeIaDKA7wO4MxvjZOY+AFsB3EhEE4joDAAXwFnxJEFEFxDR5HhI6V8DuAbAfXkyvs0AvkVEf0lEfwHg28jS7ygIMt5s/5YqAtynObtHZUzHS0Tz4zZ/ENHH4UQQZfW3jbMWjhnxfGYe8NgvL35fI5jZvtJ8AVgFZ9Uiv1bFP6uBo1LWSPt/C8A7AN4HsBHA+CyOdQqAewH0AdgP4HLpszPhmGnE+7vgRJD0AvhvANfkanyKsRGAmwAcir9uQry2WJb/9qbjzfpvaXqf5ts9GnS8AP4lPtY+AK/DMTFFsjzW2vj4DsfHJl6N+fr7mrxssT6LxWKxKLEmJovFYrEosQLCYrFYLEqsgLBYLBaLEisgLBaLxaLECgiLxWKxKLECwmKxWCxKrICwWCwWixIrICwWi8Wi5P8HLXf3ClKUH4kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1], 'go', label=\"Positive\")\n",
    "plt.plot(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must not forget to add an extra bias feature ($ \\mathbf{x}_0 = 1$) to every instance. For this, we just need to add a column full of 1s on the left of the input matrix $ \\mathbf{X} $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons_with_bias = np.c_[np.ones((m, 1)), X_moons]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.05146968,  0.44419863],\n",
       "       [ 1.        ,  1.03201691, -0.41974116],\n",
       "       [ 1.        ,  0.86789186, -0.25482711],\n",
       "       [ 1.        ,  0.288851  , -0.44866862],\n",
       "       [ 1.        , -0.83343911,  0.53505665]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_moons_with_bias[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now let's reshape `y_train` to make it a column vector (i.e. a 2D array with a single column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_moons_column_vector = y_moons.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into a training set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "test_size = int(m * test_ratio)\n",
    "X_train = X_moons_with_bias[:-test_size]\n",
    "X_test = X_moons_with_bias[-test_size:]\n",
    "y_train = y_moons_column_vector[:-test_size]\n",
    "y_test = y_moons_column_vector[-test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a small function to generate training batches. In this implementation we will just pick random instances from the training set for each batch. This means that a single batch may contain the same instance multiple times, and also a single epoch may not cover all the training instances (in fact it will generally cover only about two thirds of the instances). However, in practice this is not an issue and it simplifies the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a small batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.93189866,  0.13158788],\n",
       "       [ 1.        ,  1.07172763,  0.13482039],\n",
       "       [ 1.        , -1.01148674, -0.04686381],\n",
       "       [ 1.        ,  0.02201868,  0.19079139],\n",
       "       [ 1.        , -0.98941204,  0.02473116]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = random_batch(X_train, y_train, 5)\n",
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that the data is ready to be fed to the model, we need to build that model. Let's start with a simple implementation, then we will add all the bells and whistles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's reset the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **moons** dataset has two input features, since each instance is a point on a plane (i.e. 2-Dimensional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the Logistic Regression model. This model first computes a weighted sum of the inputs (just like the Linear Regression model), and then it applies the sigmoid function to the result, which gives us the estimated probability for the positive class:\n",
    "\n",
    "$\\hat{p} = h_\\boldsymbol{\\theta}(\\mathbf{x}) = \\sigma(\\boldsymbol{\\theta}^T \\mathbf{x})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $\\boldsymbol{\\theta}$ is the parameter vector, containing the bias term $\\theta_0$ and the weights $\\theta_1, \\theta_2, \\dots, \\theta_n$. The input vector $\\mathbf{x}$ contains a constant term $x_0 = 1$, as well as all the input features $x_1, x_2, \\dots, x_n$.\n",
    "\n",
    "Since we want to be able to make predictions for multiple instances at a time, we will use an input matrix $\\mathbf{X}$ rather than a single input vector. The $i^{th}$ row will contain the transpose of the $i^{th}$ input vector $(\\mathbf{x}^{(i)})^T$. It is then possible to estimate the probability that each instance belongs to the positive class using the following equation:\n",
    "\n",
    "$ \\hat{\\mathbf{p}} = \\sigma(\\mathbf{X} \\boldsymbol{\\theta})$\n",
    "\n",
    "That's all we need to build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n_inputs + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "logits = tf.matmul(X, theta, name=\"logits\")\n",
    "y_proba = 1 / (1 + tf.exp(-logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow has a nice function `tf.sigmoid()` that we can use to simplify the last line of the previous code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = tf.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log loss is a good cost function to use for Logistic Regression:\n",
    "\n",
    "$J(\\boldsymbol{\\theta}) = -\\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{\\left[ y^{(i)} \\log\\left(\\hat{p}^{(i)}\\right) + (1 - y^{(i)}) \\log\\left(1 - \\hat{p}^{(i)}\\right)\\right]}$\n",
    "\n",
    "One option is to implement it ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7   # To avoid an overflow when computing the log\n",
    "loss = -tf.reduce_mean(y * tf.log(y_proba + epsilon) + (1 - y) * tf.log(1 - y_proba + epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we might as well use TensorFlow's `tf.losses.log_loss()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "loss_2 = tf.losses.log_loss(y, y_proba)   # Uses epsilon = 1e-7 by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is pretty standard: let's create the optimizer and tell it to minimize the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need now (in this minimal version) is the variable intializer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are ready to train the model and use it for predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's really nothing special about this code. It's virtually the same as the one we used earlier for Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.79260236\n",
      "Epoch: 100 \tLoss: 0.34346345\n",
      "Epoch: 200 \tLoss: 0.30754045\n",
      "Epoch: 300 \tLoss: 0.29288897\n",
      "Epoch: 400 \tLoss: 0.28533572\n",
      "Epoch: 500 \tLoss: 0.2804781\n",
      "Epoch: 600 \tLoss: 0.27808294\n",
      "Epoch: 700 \tLoss: 0.2761544\n",
      "Epoch: 800 \tLoss: 0.27551997\n",
      "Epoch: 900 \tLoss: 0.27491233\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val = loss.eval({X: X_test, y: y_test})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we don't use the epoch number when generating batches, so we could just have a single `for` loop rather than 2 nested `for` loops, but it's convenient to think of training time in terms of number of epochs (i.e., roughly the number of times the algorithm went through the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each instance in the test set, `y_proba_val` contains the estimated probability that it belongs to the positive class, according to the model. For example, here are the first 5 estimated probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54895616],\n",
       "       [0.7072436 ],\n",
       "       [0.51900256],\n",
       "       [0.99111354],\n",
       "       [0.50859046]], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba_val[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify each instance, we can go for maximum likelihood: classify as positive any instance whose estimated probability is greater or equal to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_proba_val >= 0.5)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the use case, we may want to choose a different threshold than 0.5: make it higher if you want high precision (but lower recall), and make it lower if you want high recall (but lower precision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the model's precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8627450980392157"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these predictions and see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_idx = y_pred.reshape(-1) # A 1D array rather than a column vector\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that looks pretty bad, doesn't it? But let's not forget that the Logistic Regression model has a linear decision boundary, so this is actually close to the best we can do with this model (unless we add more features, as we will show in a second)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start over, but this time we will add all the bells and whistles, as listed in the exercise:\n",
    "* Define the graph within a `logistic_regression()` function that can be reused easily.\n",
    "* Save checkpoints using a `Saver` at regular intervals during training, and save the final model at the end of training.\n",
    "* Restore the last checkpoint upon startup if training was interrupted.\n",
    "* Define the graph using nice scopes so the graph looks good in TensorBoard.\n",
    "* Add summaries to visualize the learning curves in TensorBoard.\n",
    "* Try tweaking some hyperparameters such as the learning rate or the mini-batch size and look at the shape of the learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we will add 4 more features to the inputs: ${x_1}^2$, ${x_2}^2$, ${x_1}^3$ and ${x_2}^3$. This was not part of the exercise, but it will demonstrate how adding features can improve the model. We will do this manually, but we could also add them using `sklearn.preprocessing.PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enhanced = np.c_[X_train,\n",
    "                         np.square(X_train[:, 1]),\n",
    "                         np.square(X_train[:, 2]),\n",
    "                         X_train[:, 1] ** 3,\n",
    "                         X_train[:, 2] ** 3]\n",
    "\n",
    "X_test_enhanced = np.c_[X_test,\n",
    "                        np.square(X_test[:, 1]),\n",
    "                        np.square(X_test[:, 2]),\n",
    "                        X_test[:, 1] ** 3,\n",
    "                        X_test[:, 2] ** 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the \"enhanced\" training set looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00, -5.14696757e-02,  4.44198631e-01,\n",
       "         2.64912752e-03,  1.97312424e-01, -1.36349734e-04,\n",
       "         8.76459084e-02],\n",
       "       [ 1.00000000e+00,  1.03201691e+00, -4.19741157e-01,\n",
       "         1.06505890e+00,  1.76182639e-01,  1.09915879e+00,\n",
       "        -7.39511049e-02],\n",
       "       [ 1.00000000e+00,  8.67891864e-01, -2.54827114e-01,\n",
       "         7.53236288e-01,  6.49368582e-02,  6.53727646e-01,\n",
       "        -1.65476722e-02],\n",
       "       [ 1.00000000e+00,  2.88850997e-01, -4.48668621e-01,\n",
       "         8.34348982e-02,  2.01303531e-01,  2.41002535e-02,\n",
       "        -9.03185778e-02],\n",
       "       [ 1.00000000e+00, -8.33439108e-01,  5.35056649e-01,\n",
       "         6.94620746e-01,  2.86285618e-01, -5.78924095e-01,\n",
       "         1.53179024e-01]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_enhanced[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, next let's reset the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the `logistic_regression()` function to create the graph. We will leave out the definition of the inputs `X` and the targets `y`. We could include them here, but leaving them out will make it easier to use this function in a wide range of use cases (e.g. perhaps we will want to add some preprocessing steps for the inputs before we feed them to the Logistic Regression model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
    "    n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    with tf.name_scope(\"logistic_regression\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            if initializer is None:\n",
    "                initializer = tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0, seed=seed)\n",
    "            theta = tf.Variable(initializer, name=\"theta\")\n",
    "            logits = tf.matmul(X, theta, name=\"logits\")\n",
    "            y_proba = tf.sigmoid(logits)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            loss = tf.losses.log_loss(y, y_proba, scope=\"loss\")\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return y_proba, loss, training_op, loss_summary, init, saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a little function to get the name of the log directory to save the summaries for TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the graph, using the `logistic_regression()` function. We will also create the `FileWriter` to save the summaries to the log directory for TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 2 + 4\n",
    "logdir = log_dir(\"logreg\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we can train the model! We will start by checking whether a previous training session was interrupted, and if so we will load the checkpoint and continue training from the epoch number we saved. In this example we just save the epoch number to a separate file.\n",
    "\n",
    "We can try interrupting training to verify that it does indeed restore the last checkpoint when we start it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training was interrupted. Continuing at epoch 10001\n",
      "INFO:tensorflow:Restoring parameters from /tmp/my_logreg_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/my_logreg_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_logreg_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # If the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for batch_size in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch: \", epoch, \"\\tLoss: \", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can make predictions by just classifying as positive all the instances whose estimated probability is greater or equal to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_proba_val >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8627450980392157"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD7CAYAAABwggP9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO2de5QV5ZXof5uGtpvnSMvV5Bq64xpdRh6NgiY3CrJCYnyMS6N5qK3inRAMjGMykzEDixiJDMlMniYZRTE+ocfRO8FHFKOJjwRfWcEQRLgZnKXiJbSZttWO/QCaZt8/6pym+nRVnapz6pxTdc7+rVWr+1R9VbXP19Xfrr33t/cnqophGIZh5DKq0gIYhmEYycQUhGEYhuGJKQjDMAzDE1MQhmEYhiemIAzDMAxPRldagDg54ogjtKWlpdJiGIZhpIYXX3zxLVWd4nWsqhRES0sLmzdvrrQYhmEYqUFEdvkdMxeTYRiG4YkpCMMwDMMTUxCGYRiGJ1UVgzAMo7oZGBhg9+7d7N27t9KipI6GhgaOPvpoxowZE/ocUxCGYaSG3bt3M2HCBFpaWhCRSouTGlSVrq4udu/ezQc/+MHQ55mLyagOOjrg9NPhzTcrLYlRQvbu3UtTU5Mph4iICE1NTZEtL1MQRnWwahU884zz06hqTDkURiH9ZgrCSD8dHXDHHXDwoPMzjBVhFodh5MUUhJF+Vq1ylAPA4GA4K8IsDqMA6urqmDVrFtOnT+czn/kMfX19ka+xaNEiduzYAcA3v/nNYcc++tGPxiJnXJiCMNJN1nrYv9/5vH8/3HwzvPRS/nOiWBxGKmnf1k7LDS2M+sYoWm5ooX1be1HXa2xs5Pe//z0vv/wy9fX13HzzzZGv8ZOf/IQTTjgBGKkgnnvuuaLkixtTEEa6cVsPWQ4ehEsuCXdOWIvDSB3t29pZ/LPF7OrehaLs6t7F4p8tLlpJZJk7dy7/9V//BcD3v/99pk+fzvTp07nhhhsA6O3t5ZxzzqG1tZXp06dz7733AjB//nw2b97MsmXL6O/vZ9asWbS1tQEwfvx4AD73uc+xcePGoXtdccUV/PSnP2VwcJBrrrmGk08+mZkzZ3LLLbfE8l38MAVhpJvnnz9kPbjZseOQZeCON3hZHGZFVCUrnlhB38BwF1DfQB8rnlhR9LUPHDjAo48+yowZM3jxxRe54447+M1vfsMLL7zArbfeypYtW/j5z3/O+9//frZu3crLL7/MmWeeOewa//zP/zxkkbS3D1daF1100ZBC2b9/P0888QRnn302t912G5MmTeK3v/0tv/3tb7n11lt57bXXiv4+fpiCMNLNli2g6mxLlkB9vbN/zBhYtsxRDMuXH4o3eFkcZkVUJW90vxFpfxiyb/xz5sxh6tSpfP7zn+eZZ57hU5/6FOPGjWP8+PFccMEFbNq0iRkzZvDLX/6Sf/zHf2TTpk1MmjQp9H3OOussnnzySfbt28ejjz7KvHnzaGxs5PHHH+fuu+9m1qxZfPjDH6arq4tXXnml4O+TD0uUM6oDL8tg/Xpn8H/22UPxhmOOGWlx7N8PXr7fjg646CK491446qjSfwcjVqZOmsqu7pGFSqdOmlrwNbNv/G5U1bPtcccdx4svvsjGjRtZvnw5Z5xxBl//+tdD3aehoYH58+fz2GOPce+993LxxRcP3evHP/4xn/zkJwv+DlEwC8KoDvwsg9yfp59+yOJwb1u2eF/TZjqlltULVjN2zNhh+8aOGcvqBatjvc+8efN44IEH6Ovro7e3l/vvv5+5c+eyZ88exo4dy6WXXso//MM/8Lvf/W7EuWPGjGFgYMDzuhdddBF33HEHmzZtGlIIn/zkJ1mzZs3QOTt37qS3tzfW7+PGFIRRHfjFItxEiTfYTKfU0zajjbXnrqV5UjOC0DypmbXnrqVtRlus9znppJO44oorOOWUU/jwhz/MokWLOPHEE9m2bRunnHIKs2bNYvXq1Xzta18bce7ixYuZOXPmUJDazRlnnMGvf/1rPv7xj1OfcZ0uWrSIE044gZNOOonp06dz5ZVXcuDAgVi/zzBUtWq22bNnq+Fizx7VefNUOzoqLUlpyf2ee/aoNjR42Qmq9fWqS5fmv+aSJapjxjjnjBkT7hyj5OzYsaPSIqQar/4DNqvPmGoWRDVTKy6S3O/p5W7K4hdvcJO1HrKm/8CAWRFGTRKrghCRq0Rks4jsE5E787T9OxF5U0S6ReR2ETnMdaxFRJ4SkT4R+YOIfDxOOWuCWnGReH1PP3fTrFn+8QY3q1YdiltkOXCg+hWtYeQQtwWxB/gn4PagRiLySWAZsABoAY4BvuFqcg+wBWgCVgD/ISKei2obPkRNBktbbaKsvMuXj/ye7qmv+QLRXjz//CHrIcvAQH7LwzCqjFgVhKpuUNUHgK48TRcCt6nqdlV9B1gFXAEgIscBJwHXqWq/qv4U2AZcGKesVU0hyWCVcEcVo5RWrYJNm5yprHEnvW3cCA0Nw/c1NsKdd6ZLiRpGkVQqBjEN2Or6vBU4UkSaMsdeVdX3co5P87qQiCzOuLU2d3Z2lkzgVBE1GSzXTbN1a3kGwkKVUlZe1ZGuoDiS3vz6r62tNmI6hpGhUgpiPNDt+pz9fYLHsezxCV4XUtW1qjpHVedMmWJeKMDbBx8UnM11R5VjICwmRlJsEDoffv23Y0f1x3QMw0WlFEQPMNH1Ofv7ex7HssffwwhHFB+8lztq+/bSD4SFFszLlRcc909HR/RYgx9e/bdkiVO+I6q8RlUhInzlK18Z+vzd736XlStXxn6fpJQBr5SC2A60uj63An9S1a7MsWNEZELO8e1llK92CHobL9VAWEzBvGXLYN8+bzlLFWi3An/pJsbn4rDDDmPDhg289dZbMQjmT1LKgMc9zXW0iDQAdUCdiDSIiFe9p7uBz4vICSJyOPA14E4AVd0J/B64LnP+p4CZwE/jlNXIEJSB7DcQFvsPV0zBvEcecd7oc+V87rnSBdrjLPCXttli1UCMz8Xo0aNZvHgxP/jBD0Yc6+zs5MILL+Tkk0/m5JNP5tlnnx3a/4lPfIKTTjqJK6+8kubm5iEFc/755zN79mymTZvG2rVrAZJVBtwvg66QDVgJaM62EpiK4zqa6mr798CfgD8DdwCHuY61AE8D/cB/Ah8Pc3/LpC6SJUucTON8mcdLlqiOGlV4dvGsWd5ZzrNmBZ/nzpBubByeIR50rFgKldeLYvuuxomcSR3zczFu3Djt7u7W5uZmfffdd/U73/mOXnfddaqqevHFF+umTZtUVXXXrl16/PHHq6rq3/zN3+g3v/lNVVV99NFHFdDOzk5VVe3q6lJV1b6+Pp02bZq+9dZbQ/fJva+q6oYNG/Tyyy9XVdV9+/bp0UcfrX19fXrLLbfoqlWrVFV17969Onv2bH311VdHyB81k7ri5THi3ExBFEmYgbBUA3GYsiBuBZaruIKOxUGxA/uePaof/nDplFiNEFlBxPxcZAfqa6+9Vq+//vphCmLKlCna2to6tL3//e/XP//5z9ra2jpssD788MOHFMR1112nM2fO1JkzZ+rEiRP1+eefH3af3Pv29/fr0UcfrXv37tUHHnhAL7nkElVVvfDCC/XYY48dundLS4s+9thjI+Q3BWGUllINxPkGYK/6StlBNuhYHMShFJcscc4fNcq779wKslZqaBVAJAVRguciO1B3dXVpc3Ozrly5ckhBNDU1aV9f34hzZs6c6akgnnrqKT311FO1t7dXVVVPP/10feqpp4bdJ/e+qqqXXnqpPvjgg3rxxRfrQw89pKqqF1xwgf785z/PK7/VYjJKh1+wtti8iTBTXoPiAKVeBChoxlWYmEJHB9yeKS6QvU5ufMftJ6+VGlqlpoTPxeTJk/nsZz/LbbfdNrTvjDPO4F//9V+HPmfXjTjttNO47777AHj88cd55513AOju7ubwww9n7Nix/OEPf+CFF14YOjcxZcD9NEcaN7MgSoxfjGLatOLcL2GskiD3V5wxglzyvYWGcT1l2/hVlnXfo6FB9bDD4reCqoRIFkQJngv3m/ybb76pjY2NQxZEZ2enfvazn9UZM2bohz70Ib3yyitVVfVPf/qTfuxjH9MTTzxRv/zlL+v73vc+3bt3r+7du1fPPPNMnTFjhn76058eZkF89atf1eOPP37IheS+7/79+3Xy5Ml6xRVXDO0bHBzU5cuX6/Tp03XatGk6f/58fffdd0fIby4mw5+sH/wjHyls4PH7hxMpfEArtXuoWIIC92FcT0Glx7ODlfseo0b5u6GMVJb73rt3rw4MDKiq6nPPPaetra0Vk8VcTIY/q1bBb34DL7xQmJldigSypK8RHZSVHibZz+v71dfD0qVO/23cONxtd/CgvxvKSCVvvPEGJ598Mq2trVx99dXceuutlRYpPH6aI42bWRAB7NlzyHWRdWUU+5Yex9t/Kd1DpSTsd8/3/bwslFxrZeFCC1pnSKMFkSTMgjC8WbVqeAnrffvgpJOKezuN4+2/2NLclSLsd8/3/fItlbp/Pzz88KGgtSXa4YxpRlQK6TdTELVAdhaNe0BTdfYvW1b4daMWBawm4vrufgoku+3ZA729h2Z4LV9e0zOcGhoa6OrqMiUREVWlq6uLhtwy9nmQauroOXPm6ObNmystRvJYuhRuucW75lJdHezeDUcdVX65qoVs/37xi3DjjfFf+7bbHOVTX+9YKYODToHCV1+tub/bwMAAu3fvZu/evZUWJXU0NDRw9NFHMyYbM8wgIi+q6hyvc0xB1AInngiZOdmeLF0a/8CWZDo64KKL4N57ix9gOzrgmGNg7974B233tXOpr4dFi2rr72aUhCAFYS6mpFBK37LbjbFnz8jV0rIzZWrFvx1nIlqhZcujXjsXm+FklAFTEEmhXNmzQeWyayGDt5CFivwUZ6nLgOcLYCdpOrBRlZiCSALFrK4WFb9y2b/6VflkqCSFvPH7Kc5S53Bs2eLkmYwaBU1NI4/XyoQAo2KYgkgCpXBTeL31dnQ4M2Jg5Cps8+aVzlWSFAp54w9S3qWexeW+d1/f8L9Xdtu40fvvXAuuQqP0+CVIpHFLZaJcsclmfpU/vWoE+dU8Snq5i7gIu96F3znlLn3hvreIkzDn1cbr72xrThghwWoxJZhCBq3c83MHA68aQUFKoFgZ0kLUrO1KKk6ve9fV5V8kqZQLJxlVSZCCiHvJ0ckicr+I9IrILhG5xKfdoyLS49r2i8g21/HXRaTfdfzxOOVMFMW4KfzcH14uqyB/ea0kvEXN2q5knSi/e7sTG/P9navVVWiUDz/NUcgG3APcC4wHTgO6gWkhznsa+Lrr8+uEXGbUvaXSgigGL/eH31vvtGnR3p6NytaJ8rv3EUc4x73+zg0NteEqNGKFAAtidFyKRkTGARcC01W1B3hGRB4CLgN86zmISAswF/jfcclSE/gFXLNlGdwMDjpBy5dfLr+caaaS9aC2bPFOlOvtdSxFLwvDa0rs4KBTc+t3v6u5rGujeOJ0MR0HDKrqTte+rcC0POddDmxS1ddy9reLSKeIPC4irX4ni8hiEdksIps7OzsLkzyN+LkgHn64NtxFtUBUt6C7VHiW/fsdRWOuJqMA4lQQ43FcSm66gQl5zrscuDNnXxvQAjQDTwGPichfeJ2sqmtVdY6qzpkyZUpUmdOLX9zgAx9IZ3VUYyRBsaF8Rf40J2u+mnNbjJIRp4LoASbm7JsIvOd3goicBhwF/Id7v6o+q6r9qtqnqt8C3sVxQxlZ0lom2whPsX9jC1gbRRKngtgJjBaRY137WoHtAecsBDZkYhZBKCBFyle9WGKUkUupy4AYNUFsCkJVe4ENwPUiMk5ETgXOA9Z5tReRRuAz5LiXRGSqiJwqIvUi0iAi1wBHAM/GJWvVUQs1lIxwZF8Wli9P9lKuRiqIu9TGUqAR+G+cKa9LVHW7iMwVkVwr4XycGMVTOfsnAGuAd4A/AmcCZ6lqV8yyVgflrONkJJ/sy8Ijj9hkBaNobD2ItJO7oIytEVC7lHJtCqNqsfUgqhXzMxtuyhGUtnhXTWEKIs1UshSEkSzK9bJg8a6awhREGvB7a6uVGkpGfsrxsmDxrprDFEQa8Htryy4oU1/vfK6vd2ISlgtRe5TjZcHyKmoOC1InnaDAo1etHgtO1hYdHXDRRXDvveH+5lHbu8+zZ60qsSB1mgl6a7MYhBE1JlBoDCHoWbPAddViCiLJ5As8WgyitokaEygmhhD0rFngumoxBZFUOjpg9uxgC8HqMdU2UWMCxcQQ/J61jRstcF3FmIJIKlnT3SwEw4sg69LL5VOqabAWuK5qTEEkkew/MziBwI4Op3TzvHnO72YhGEExAS+XTyniVZaoWfWYgkgifmsNe/l5LUBYm/jFBH71K2+XT1AModBnyCZJVD2mIJKG11vZ7bf7+3ktQBidalCqfjGBefOGv1ycdJLzPTduPLR4UJbGRnj00cKfIZskUfVYHkTScBffyzIqo8cPHhxekM+KsxXG0qVwyy1w2WXw2mvRcwKSileuAsAVVzjPR+5zVV8PF1/sfH97hmoWy4NIE/nWGnb7eS1AGB33VM/162HTpurpNy+XD8C6dfDrX3u/7T/8sD1Dhi+mIAqhlC6KXNeBu5RGlsFBWLbMAoSFkKtUVaun37xeLsD5nqefPnK96o98BHp6Rj5DW7em3wVnxIIpiEKIO2AcdJ6fn9f95pfF3gCDyY3vZKmWfsu+XOzZMzLe4BW7euEFGBgY3m5wENraLK5lAKYgohOUjRol2OdWCu7zcpWFXzDyAx+wAGFU/Fww1WZ95Ztd5J5Gndtu/37YscMS3wwHVY1tAyYD9wO9wC7gEp92K4EBoMe1HeM6Pgt4EejL/JwV5v6zZ8/WkrNkiWp9vTNM19erLl3q7N+zR7Whwdnf2Kja0ZH/OqNGqS5cOPy8hQud/dnrhpEnSvtaZtYsL1U78m+Zdvy+56xZznG/ZzjfMaMqATar35jud6CQDWcd6nuB8cBpOGtOT/NotxJY73ON+oxy+TvgMODqzOf6fPcvuYJwK4HsllUGUf6x3Nepq1MdM8b5fcwY53NYJRNVKRkO+QbQaiboGQ46Fvba8+bZc5gyghREbC4mERkHXAhcq6o9qvoM8BBwWcRLzQdGAzeo6j5V/REgwMfikrVg/Ez3qAHj3EBp1g88MOB8zu4vZW2dWqaWa1jly8AuJq5lOTlVR5wxiOOAQVXd6dq3FZjm0/5cEXlbRLaLyBLX/mnASxnNluUlv+uIyGIR2Swimzs7O4uRPz9xBIz9AqW55FMyVubAKISg5LZiEt9stbmqJE4FMR7HpeSmG5jg0fY+4EPAFOALwNdF5OICroOqrlXVOao6Z8qUKYXKHo44AsZ+gVIvgt7erMyBUQhB1tPGjYfqfUW1rMyarUriVBA9wMScfROB93IbquoOVd2jqoOq+hzwQ+DTUa+TGKK4LPzmqudOS4TgtzcrcxCdaiixUUoKdRGZNVu1xKkgdgKjReRY175WYHuIcxUnzkCm/UwREdfxmSGvk3z8lEl/fzS/eC360Ysd4M1H7k8xLiKzZquW2BSEqvYCG4DrRWSciJwKnAesy20rIueJyOHicArOTKUHM4efBgaBq0XkMBG5KrP/ybhkTRT2VhueYgZ485EHU4yLyKzZyLRva6flhhZGfWMULTe00L6tvdIieeM3vamQDScP4gGcPIg3yORBAHOBHle7e4AuHHfSH4Crc65zIk7+Qz/wO+DEMPcvSx5E3FgeQziKndJr8/v9KXZ6qxGJ9S+t17GrxyorGdrGrh6r619aXxF5CJjmatVcK4lVYw2Pu8qtu6JtGLyqnFp/H8KrgnDUPjZC03JDC7u6d43Y3zypmde//HrZ5bFqrknFZn6Eo9ggqPnIgzEXUVl5o/sNz/27unclztVkCqJS2MyP8BQ7wNsAGEx2wsOSJc7aI0uXVv+EhwoyddJU32OLf7Y4UUrCFESlsLfa8BQ7wNfijK+oWBC/bPzl5L/0PdY30MeKJ1aUUZpgTEFUCnurDY8N8KXH3J1loX1bO0++Fjwhc1f3rsTMbrIgtWHUOmGD+B0dcNFF1bNEawXwC1D7IQiK0jypmdULVtM2oy12mSxIbRiGP2HdnR55KKmZz58Q/ALUfijOC/yu7l0ViU+YggBLVjNqmzDuTo8YRfu2dhb/bDG7unehaMUGsTQRFKDORyXiE6YgwEowGLVNmBiPR4xixRMr6BvoG3appAVZk8bqBasZO2bssH2CsGTOEponNec9P6oFUiymIGz2RvVilmEgod1DPlOy9+329qWXexBLE20z2lh77lqaJzUjCM2Tmll3wTpuOucmVi9YzZhRYwLPL8YCKQRTEDZ7o3oxy9CXSO4hnxjFv7ww3vPa5R7E0kbbjDZe//LrHLzuIK9/+fVhgefhNUqHM3bMWFYvWF0OEYeobQVhyWrVi1mGgURyD/nEKP7qraYR7pJKDGLVwoonVrB/0HshseZJzaw9d21JZjEFUdsKwpLVqhezDAPxcwO59w+5oM7fSssPmml/af2wGMXkP7w+wl1SiUGsWvD7mwgywtIoF6PLfsckYclq1YmfZXjttTZ/P8PUSVM95+Nn3UNZF1TWysi6oIBhA1XbjDZTCDGR729SCWrbgrAM3erELMO8eM2mcbuHbIZS4RSaG5Lvb1IJaltBGNWJWYZ58ZpN43YPhXFBGSMpJjck39+kElipjVJhZQmMFJO0NQvSQhr7rWylNkRksojcLyK9IrJLRC7xaXeNiLwsIu+JyGsick3O8ddFpF9EejLb43HKWRZsiqWRYpLo7kgD1WZ5xe1iuhHYDxwJtAFrRGSaRzsBLgcOB84ErhKRi3LanKuq4zPbGTHLWVpsiqWRcrLujqbGpqF9jaMbI1+n1mo1+QWU05obEpuCEJFxwIXAtarao6rPAA8Bl+W2VdVvq+rvVPWAqv4n8CBwalyyVBybYmlUCf0H+od+7+rvilRrqRZrNVWb5RWnBXEcMKiqO137tgJeFsQQ4qQOzgW25xxqF5FOEXlcRFoDzl8sIptFZHNnZ2ehsseHJd8ZVUKxM5lqcSZUEgPNxRCnghgPdOfs6wYm5DlvZUaOO1z72oAWoBl4CnhMRP7C62RVXauqc1R1zpQpUwoQO2bimGJpNYSMBFCsP73a/PFhCSqlkTbiVBA9wMScfROB9/xOEJGrcGIR56jqvux+VX1WVftVtU9VvwW8i2NlJJ84plhagNtIAMX606vNH1+LxKkgdgKjReRY175WRrqOABCRvwaWAQtUdXeeaytOYDv5bNwI8+Y5VoB7EfiwyXcW4DYSQrH+9Grzx9cisSkIVe0FNgDXi8g4ETkVOA9Yl9tWRNqAbwKfUNVXc45NFZFTRaReRBoyU2CPAJ6NS9aSkn37X7assIHeAtyFEdYtZ+670BTrT682f3xNoqqxbcBk4AGgF3gDuCSzfy7Q42r3GjCA45bKbjdnjk0DXspcowt4ApgT5v6zZ8/WirJnj2pDg1Owo65OdcwY5/f6etWlS6Odn90aG1U7Okove9pZskR11Kjgft6zR/V971MVCff3MGJh/UvrtfkHzSorRZt/0KzrX1pfaZEMF8Bm9RvT/Q6kcau4gliyxFEGXhWeRFS3bo1+fljlUsu4FWuQQr38clO8ESl2cF//0nodu3qsspKhbezqsaYkEkSQgrBaTHGRO701F1W4xDOx/BBWQygaWXfR8uX53XIdHdDumn9/4IC57/KQL48hTBJcLU51rSasFlNcLF0Kt93mryAARGDPHqvNFBdLl8LNNzsTAQYHD+1vbIRXXx3ezwsXwt13Dz/fq50xRFBdodULVg8rBw5OADo3xjDqG6NQRo4xgnDwuoMj9hvlp2y1mGoar7f/LGPGHPppb63x8PvfO8pBdbhygJHWQa714NfOGEZQHkNYy8CmuqYbUxBx4bW2xHHHOccGBpyf2azqrVttJk2xXHqp08deDAwMd8utWjVSiXi1M4YRNLiHTYKzqa7pxhREqfjFL2DnzpH7Bwehrc0S4Yrh97+H7cPTa/rqoL8u8/to+On3Fw0de/vJjd7XmTXLFocKIGhwn9w42fOc3P021TXdWAyiVEyeDO+8431MxHn7NR94YUyfPkJBHAAQGK2wtw7u+8h4Ln/mvRFLZ4K3r9zwpn1bOyueWMEb3W8wddJUVi9YTduMNo749hF09XeNaN/U2MRbX32rApIahWIxiHLzi194K4cnnnCyq7MxCUuEi05HB+zYMWL3aBzlANAwCJ/+TQ+8+abNoikRb/e/7bm/q7+rJsp61wqmIErB5z7nvf+CC/wrvVqGbzhWrTqkYDMcAA7kFGKpU6dtkK+81tYqiErQNNegIHMtlPWuFUxBxE1Hh79rqbvbv9KrFegLh8dsMbf1kOWwQeC553wHssmNk2turYKoBFlfXvEJr3ZGujEFETerVkF9vf9xr0S4X/3KCvSFxTVbrP2l9YxbPRZZydA2aqWw9OElTpstW3wDrYC5nvIQZH25g89RzzfSgymIuAnKhwBn5kzudNh586xAXwF4zZBZd8E6bjrnpsA2a89d6+tDt0HtEPlyGLLrHvgpCct1SD+mIOIm+4a7ZMkhS6K+3sn6zbzVDsNWoCuKMIuz5LYBGCXej74NaocIm8NguQ6FkYYYmCmIUhBl0I9jBTojNNnA66COTJzLHdTS8A9cSsLmMFiuQ3TSsl635UGUAq+6TPX1sGgR3Hjj8LYnnugkfuViSVwlwa++UJ3Ucden7hoa1Cx/wiglQXWuslZuuQjKgzAFUQps0E8sfsXjwPnnzCaE9ezvsUQwo2QkqYihJcqVG6+6TF7xB6Ps+MUYBBlm7nspB3ASwZLmBjDSR1xFDEvtBjUFYdQUXgFVQXytCi9sKqxRLHEE9ssRx4hVQYjIZBG5X0R6RWSXiHiukCMO/yIiXZnt2yIiruOzRORFEenL/JwVp5xG7ZIbUG1qbIqkHABP33GtUesB/GLxC+wDofu1HGVk4rYgbgT2A0cCbcAaEZnm0W4xcD7QCswE/gq4EkBE6oEHgfXA4cBdwKy5ctAAABUqSURBVIOZ/YZRNNlpr+suWEf/gf7I59dJXQmkSg9pmYGTdLymX0fp17Al14shNgUhIuOAC4FrVbVHVZ8BHgIu82i+EPiequ5W1T8C3wOuyBybj1M94QZV3aeqPwIE+FhcshoGeL+BZQkqI+E1RbaWsAKIpSFqv5ZjMaY4LYjjgEFVdS+CsBXwsiCmZY55tZsGvKTDp1e95HMdRGSxiGwWkc2dnZ0FC2/UHkFvWkFlJILKS9QC5XhzrUWi9ms5EhTjVBDjge6cfd3AhBBtu4HxmThElOugqmtVdY6qzpkyZUpBghu1id+bVvOkZtpmtFmGsA+2jGhpiNqv5UhQjFNB9AATc/ZNBN4L0XYi0JOxGqJcxzAKJp8CiPIPWEtBW1OcpaGQfg1TaqYY4lQQO4HRInKsa18rsN2j7fbMMa9224GZ7llNOIFsr+sYxgjCDtZhFECYf8BaC9paaY3SkMR+jTWTWkT+HVBgETAL2Ah8VFW357T7IvAl4OOZ9r8AfqyqN2dmK70CfB+4GfgCcA1wrKoGlElNUCa1UTEqUSIjSWUTDCMq5cykXgo0Av8N3AMsUdXtIjJXRHpc7W4BfgZsA14GHsnsI6MEzgcuB94F/ho4P59yiB1b4S2VVGKGjQVtjWpldJwXU9W3cQb33P2bcILP2c8KfDWzeV1nCzA7Ttki417hLbfAnpFYKjFYT5001dOCsKCtkXas1IYX2XLdtsJb6qjEDBsL2hrViikIL9xrNHitzdDRAR/5CPyv/2XKI2FUYrBOYnDRMOLAyn3n0tEBxxwDe/ce2tfYCK++Ckcd5XxeuhTWrDn0u7mgEkX7tnZWPLFiqHT36gWrbbA2DB9sPYgo5Fvsp6MDPvhB2LfPOdbQAK+9dkh5GIZheJDUFxdbDyIKzz8/XDmA8/m555zfV62CgYHhx2x50Jon7YlyaZc/6aQ1V8YsiCjkWg9ZzIqoadK+PGna5S8nhVoBSc6VMQsiLnKthyxmRdQ0aa9umnb5y0UxVkBac2VMQUTh+ecPzW5yc/DgIReUUXOk9Z8/i5+cu7p3mbvJRTGK1G+ataKJ7mNTEFHwW2va1puuadJe3TRIzrT4yoslTAymmBcBr+nXWZLcx6YgDKNICs29SEpgOGjwgnS5mwrp07CuIz9FOkpG5b2PO1fGi6T2sSkIwyiSQhLlkjSrJd/gBelwlxXap2FdR36KdFAHQ90nWxlYEM/jSexjm8VkGBUgqbNakipXGKLKnp2R5HUOgCAcvO7giHMW3r/Qc9nZsH2UtD62WUyGkTCSGthOc12pKH3qtjb88HIptc1o46B6TFQJuH8uaepjUxCGETNh/OBJDWynua5UlD71ciu5CRqwi/3bpamPzcVkGDESNunMktPiJ0qfjvrGKBTvsa95UnNgAly1/e3MxWQYZSJswDNNb5FpIUqf+r3tZ+MAQX+HWvrbxWJBiMhk4DbgDOAtYLmq/ptP22uAhUBzpu1Nqvod1/HXgSOBbBToOVU9I4wcZkEYlcbvzdQr4GlUjjisgKQW34tKOSyIG4H9OAN7G7BGRKb5yYOznOjhwJnAVSJyUU6bc1V1fGYLpRwMIwkkNbZgDKdYKyDMlNqk5LkUQ9EWhIiMA94Bpqvqzsy+dcAfVXVZiPN/lJHjbzOfXwcWqeovo8piFoRRaarNP214k2+qapqeg1JbEMcBg1nlkGEr4GdBuAUTYC6wPedQu4h0isjjItKa5xqLRWSziGzu7OyMKrthxEqhSXNpf9NMO1H/Bvmm1FZLAcQ4LIi5wP9R1aNc+74AtKnq/DznfgM4HzhFVfdl9p0K/A7HFfWlzHa8qr6bTxazIIy0kaY3zWrDnSgnyLDYUZ3UoSgH9SB1Usfi2Yu56Zybho7nsyDSFIsqyoIQkadFRH22Z4AeYGLOaROB9/Jc9yqcWMQ5WeUAoKrPqmq/qvap6reAd3GsDMNIPFHfRKvlTTNt5CbK5Q7mgzo4lBA3qIOs2byGpY8sHTqeL9ktqG5TmizFvApCVeerqvhspwE7gdEicqzrtFZGuo2GEJG/BpYBC1R1dz4RwKd4iWGUkXyDfyG1gJKaUV3t5EuU82Lti2uHfs/nSgyq21Tp2ltRKDoGoaq9wAbgehEZl3ERnQes82ovIm3AN4FPqOqrOcemisipIlIvIg2ZKbFHAM8WK6dhFEOYwb8Qa6Bcs54szjGcQhRwbv2lbPG9g9cdHJE7katA6qRuxPXSYCnGNc11KdAI/DdwD7BEVbeDE6MQkR5X238CmoDfikhPZrs5c2wCsAZnVtQfcabBnqWqXTHJaRgFEWbwL8Qa8HNVnH3s2bEN6EmqHOtFJZRXIQrYrwqrH24FUmz9pkoRi4JQ1bdV9XxVHaeqU91Jcqq6SVXHuz5/UFXHuPIcxqvqFzPHtqvqzMx1mlR1gapa1NmoOGEG/0KsAS9XxcLWhdy19a7YBvRKxTnCDPyVUl751sDwYlz9uILvl9b8GCu1YRghCPMPfvaxZ3u28dufJddVsfGVjbEO6JWIc4Qd+CulvHIVc1NjE/V19YHn9O7vLfh+aarg6sYUhGGEIMw/+MZXNnqe67ffj7gH9DDKLW43T9iBv5JBerdiHl8/nv2D+wPbe/VjmIkLLTe0cNmGy2gc3UhTY1Oq6jeZgjCMEIRJgMs32IUdhIt1R+Te5+xjzw5UbqVw84Qd+KN+11LFK/IpJK+3/Xz9lnu8q7+L/gP9rLtgXd6CgEnBFIRhhCRo1goED3ZRBuFi3BFe97lr610sbF3oq9xK4eYJO/BH+a6ljFcEKV+/t/18/VYNOS6mIAwjJoIGuyiDRTGF5Pzus/GVjb7KrRRunrADf5TvWsoB10/e9Res933bz9dv1ZDjMrrSAhhGteB+I88tAX3Zhss8z/EbLNpmtBXkgihkUJo6aapn2YhiZtgE9YVX2zDf1e877OreRfu29qJcNlHkzZKv30rRr+XGVpQzjDJQroXq/e7T1NjE+PrxnoNfWupB+X03qIy8+fotLf1qK8oZRoUpNK4QNSjrdZ/6unr+vO/Pw3z3l224bKi2UFpWSAvKXaiEbz9fv6WlX4MwC8IwykTUFcgKfQPNvU/P/h66+kcWIxCEdResS9WA1b6tnUs3XOp5LImVUtNAkAVhCsIwEkpcbim/0tOFXCsJlMtdVyuYi8kwUkhQUDZKDkBQULQSM2qKzWUol7vOMAVhGIklaGCPkgOwesFq30Jz5ZxR076tnSO+fQSXbri0qFyGQlfty5fUZspjJOZiMoyE4hWDyCWsW2XpI0tZs3nNsH31dfXcft7tZYlB5PsucbuHwsZhmic1s3rB6lTMNioV5mIyjJTSOLox8HhYF9GpU09lzKgxw/aV8+Uw3wI9cbq6vKwFL+WQvW81ZDyXClMQhpFAsoOc38CWJayLaMUTKxg4ODBs38DBAb706JcKljEK+RRAnK6uKKvFTZ00tSoynkuFKQjDSCBhBrncgntBPnS/wa6rvwv5hpTc7x6kAIope+31vcMO7Nn7pnWthnJgCsIwEkjQIJcbmA1TxC7fYFfqhXpWL1jtu95C1p0T9d5+33ty42TP9k2NTZ6B7bSu1VAOYlMQIjJZRO4XkV4R2SUilwS0XSkiA64lR3tE5BjX8Vki8qKI9GV+zopLTsNIA34DevOk5hEF98L40MMMdn0DfSy8f2FJlETbjDYm1E/wPV6IgvL73oDngP/Ds37oWbCwGjKeS0WcFsSNwH7gSKANWCMi0wLa35uz7OirACJSDzwIrAcOB+4CHszsN4yaIMpbbRgfetuMNpoam/Led1AHSzb98+3+twOPRw0M+33vt/vfjjzg5yvlXqvEoiBEZBxwIXCtqvao6jPAQ4B3Cctg5uNUmb1BVfep6o8AAT4Wh6yGkQaivNWG9aH/8KwfhlqH2e3yiXP9hTA+/SiB4aDvbQN+PMRlQRwHDKrqTte+rUCQBXGuiLwtIttFZIlr/zTgJR0+B+8lv2uJyGIR2Swimzs7OwuV3zASR9hBrpC1FwDf5DkozfTPoGJ7WaIEhi12UHriUhDjge6cfd2An9PxPuBDwBTgC8DXReTiQq6lqmtVdY6qzpkyZUohshtGqolibWSVjl6nrLtgHXVS53nNOKd/eq3LDCMVVNTB3WIHpSfUgkEi8jRwus/hZ4G/BSbm7J8IvOd1gqrucH18TkR+CHwauAfoiXItwzAKW2Ao294rizi7Cl6xC97kZlB39XcNrdQG0Rbo8fsOphBKRygFoarzg45nYhCjReRYVX0ls7sV2B5SDoWh14ntwFdERFxuppk4QXDDqCmilgiPSr6V1PyUR1iC3FQWG0g+sbiYVLUX2ABcLyLjRORU4DxgnVd7ETlPRA4Xh1OAq3FmLgE8DQwCV4vIYSJyVWb/k3HIahhpIe4gsR9+sY44XDhpyFK2Qn3+xFasT0QmA7cDnwC6gGWq+m+ZY3OBR1V1fObzPcAZwGHAbuCmzGyl7LVOBH4CnAD8X+DzqrolnwxWrM+oJqph3YO4vkOpLKm0LAtaSmzBIMNIAbmDoN/6y2laOS2OAbiUg3g1KOFisWquhpFwvNxJSVjDoVjicFOVstpqGlxglSRUkNowjNLiNQgqiiDDlgtN8jx/PzdQsTONSjmI+1lqaVLCpcQsCMNIAH6DnaKpmOdfyoB6KautWrJdMKYgDCMBBBXnS0LJiHwzffzcQHEU/yvlIG7JdsGYi8kwEoDfspdJeJPNDRJnrQM4lEfhZwFli/+520YlX65GsViynT82i8kwEkKpk+IKJcxMH782Xm2NZGGzmAwj4SRVOUC4IHG+QnyVnhVkyXCFYQrCMCpMuTKmCyVMkDjry/cr/jdKRlXs+yS9f5OMKQjDqDClnOcfB1HKid/1qbs8LYnchYjKSdL7N8mYgjCMCpP0ZK2o5cT9LIlKDcpJ798kY7OYDKPCpCFZK8pMn7YZbVy2wXsxyUoMymno36RiFoRhVJhqTNYqZXJbVKqxf8uFKQjDqDDVmKyVpEG5Gvu3XFgehGEYJSHJU3eNQ1i5b8MwDMMTS5QzDMMwIhOLghCRySJyv4j0isguEbkkoO2jItLj2vaLyDbX8ddFpN91/PE4ZDQMI9lYtnPyiGua643AfuBIYBbwiIhsVdXtuQ1V9Sz3ZxF5mpHrTZ+rqr+MSTbDMBJOmIKARvkp2oIQkXHAhcC1qtqjqs8ADwHeE6GHn9sCzAXWFSuHYRjpxbKdk0kcLqbjgEFV3enatxWYFuLcy4FNqvpazv52EekUkcdFpDXoAiKyWEQ2i8jmzs7OaJIbhpEILNs5mcShIMYD3Tn7uoEJIc69HLgzZ18b0AI0A08Bj4nIX/hdQFXXquocVZ0zZcqUsDIbhpEgkpRYZxwir4IQkadFRH22Z4AeYGLOaROB9/Jc9zTgKOA/3PtV9VlV7VfVPlX9FvAujhvKMIwqJUmJdcYh8gapVXV+0PFMDGK0iByrqq9kdrcCIwLUOSwENqhqTz4RAMknp2EY6aXUq8YZhRFLopyI/DvOQL4IZxbTRuCjXrOYMu0bgQ7gAlV90rV/KvAB4Lc41s3fAl8FjlfVrnxyWKKcYRhGNMqRKLcUaAT+G7gHWJJVDiIyV0RyrYTzceIUT+XsnwCsAd4B/gicCZwVRjkYhmEY8WKlNgzDMGoYK7VhGIZhRMYUhGEYhuGJKQjDMAzDk6qKQYhIJzBybcHycgTwVoVlKBSTvXKkWf40yw7plj8O2ZtV1TPLuKoURBIQkc1+AZ+kY7JXjjTLn2bZId3yl1p2czEZhmEYnpiCMAzDMDwxBRE/aystQBGY7JUjzfKnWXZIt/wlld1iEIZhGIYnZkEYhmEYnpiCMAzDMDwxBWEYhmF4YgqiCETkqsxyp/tE5M4Q7f9ORN4UkW4RuV1EDiuDmEHyTBaR+0WkV0R2icglAW1XisiAiPS4tmOSKK84/IuIdGW2b4tIRdcUiSB7xfvZQ6bQz3kCn/FQsovIFSIymNPv88snqadMh4nIbZnn5T0R2SIiZwW0j73vTUEUxx7gn4Db8zUUkU8Cy4AFOEuqHgN8o5TCheBGYD9wJM5Sr2tEJGgt8XtVdbxre7UsUh4irLyLcUrKtwIzgb8CriyXkD5E6etK93MuoZ7zhD7jof9Hgedz+v3p0oqWl9HA/wNOByYB1wL3iUhLbsNS9b0piCJQ1Q2q+gAQZr2KhcBtqrpdVd8BVgFXlFK+IDIrAV4IXKuqPar6DPAQcFmlZAoiorwLge+p6m5V/SPwPayvCybCc56oZxwi/48mClXtVdWVqvq6qh5U1YeB14DZHs1L0vemIMrHNGCr6/NW4EgRaaqQPMcBg6q6M0emIAviXBF5W0S2i8iS0oo3gijyevV10PcqNVH7upL9XAxJe8ajcqKIvCUiO0XkWhHJuyRzORGRI3GeJa+VOkvS96Ygysd4nFX0smR/n1ABWWCkPGQ++8lzH/AhYArwBeDrInJx6cQbQRR5vfp6fAXjEFFkr3Q/F0PSnvEo/BqYDvwPHGvvYuCaikrkQkTGAO3AXar6B48mJel7UxA+iMjTIqI+2zMFXLIHmOj6nP39veKlHUkI+XPlycrkKY+q7lDVPao6qKrPAT8EPl0K2X2IIq9XX/do5bJCQ8uegH4uhrI+43Giqq+q6msZV8424HoS0u8iMgpYhxPDusqnWUn63hSED6o6X1XFZzutgEtuxwmaZmkF/lSq9bZDyL8TGC0ix+bI5GW+et4CKOcbeRR5vfo67PcqBcX0dbn7uRjK+oyXmET0e8bqvQ1ncsOFqjrg07QkfW8KoghEZLSINAB1QJ2INAT4Le8GPi8iJ4jI4cDXgDvLJOoIVLUX2ABcLyLjRORU4DycN5URiMh5InJ4ZgrpKcDVwIMJlfdu4O9F5H+KyPuBr5CSvq50P3sR4TlP1DMO4WUXkbMyPn5E5HicGUMV7fcMa3Bcjueqan9Au9L0varaVuAGrMR503BvKzPHpuKYfVNd7f8e+BPwZ+AO4LAKyz8ZeADoBd4ALnEdm4vjlsl+vgdnJkgP8Afg6qTI6yGrAN8G3s5s3yZTdyxpfZ3EfvaQ3fM5T8kzHkp24LsZuXuBV3FcTGMqLHtzRt69GVmzW1u5+t6K9RmGYRiemIvJMAzD8MQUhGEYhuGJKQjDMAzDE1MQhmEYhiemIAzDMAxPTEEYhmEYnpiCMAzDMDwxBWEYhmF48v8Bdx3ubN6sKMYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_idx = y_pred.reshape(-1)  # A 1D array rather than a column vector\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's much, much better! Apparently the new features really helped a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's try starting the tensorboard server, find the latest run and look at the learning curve (i.e., how the loss evaluated on the test set evolves as a function of the epoch number):\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir=tf_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can play around with the hyperparameters (e.g. the `batch_size` or the `learning_rate`) and run training again and again, comparing the learning curves. We can even automate this process by implementing grid search or randomized search. Below is a simple implementation of a randomized search on both the batch size and the learning rate. For the sake of simplicity, the checkpoint mechanism was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "   logdir:  tf_logs/logreg-run-20201129104611/\n",
      "   batch_size:  52\n",
      "   learning_rate:  0.004430375245218265\n",
      "   training: .....................\n",
      "   precision:  0.9797979797979798\n",
      "   recall:  0.9797979797979798\n",
      "Iteration:  1\n",
      "   logdir:  tf_logs/logreg-run-20201129104824/\n",
      "   batch_size:  73\n",
      "   learning_rate:  0.0017826497151386947\n",
      "   training: .....................\n",
      "   precision:  0.9696969696969697\n",
      "   recall:  0.9696969696969697\n",
      "Iteration:  2\n",
      "   logdir:  tf_logs/logreg-run-20201129104952/\n",
      "   batch_size:  6\n",
      "   learning_rate:  0.00203228544324115\n",
      "   training: .....................\n",
      "   precision:  0.9797979797979798\n",
      "   recall:  0.9797979797979798\n",
      "Iteration:  3\n",
      "   logdir:  tf_logs/logreg-run-20201129110549/\n",
      "   batch_size:  24\n",
      "   learning_rate:  0.004491523825137997\n",
      "   training: .....................\n",
      "   precision:  0.9797979797979798\n",
      "   recall:  0.9797979797979798\n",
      "Iteration:  4\n",
      "   logdir:  tf_logs/logreg-run-20201129111049/\n",
      "   batch_size:  75\n",
      "   learning_rate:  0.07963234721775589\n",
      "   training: .....................\n",
      "   precision:  0.9801980198019802\n",
      "   recall:  1.0\n",
      "Iteration:  5\n",
      "   logdir:  tf_logs/logreg-run-20201129111240/\n",
      "   batch_size:  86\n",
      "   learning_rate:  0.0004634250583294876\n",
      "   training: .....................\n",
      "   precision:  0.912621359223301\n",
      "   recall:  0.9494949494949495\n",
      "Iteration:  6\n",
      "   logdir:  tf_logs/logreg-run-20201129111417/\n",
      "   batch_size:  87\n",
      "   learning_rate:  0.047706818419354494\n",
      "   training: .....................\n",
      "   precision:  0.98\n",
      "   recall:  0.98989898989899\n",
      "Iteration:  7\n",
      "   logdir:  tf_logs/logreg-run-20201129111552/\n",
      "   batch_size:  61\n",
      "   learning_rate:  0.0001694044709524274\n",
      "   training: .....................\n",
      "   precision:  0.8969072164948454\n",
      "   recall:  0.8787878787878788\n",
      "Iteration:  8\n",
      "   logdir:  tf_logs/logreg-run-20201129111757/\n",
      "   batch_size:  92\n",
      "   learning_rate:  0.04171461199412461\n",
      "   training: .....................\n",
      "   precision:  0.9797979797979798\n",
      "   recall:  0.9797979797979798\n",
      "Iteration:  9\n",
      "   logdir:  tf_logs/logreg-run-20201129111928/\n",
      "   batch_size:  74\n",
      "   learning_rate:  0.00010742922968438615\n",
      "   training: .....................\n",
      "   precision:  0.8837209302325582\n",
      "   recall:  0.7676767676767676\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "\n",
    "n_search_iterations = 10\n",
    "\n",
    "for search_iteration in range(n_search_iterations):\n",
    "    batch_size = np.random.randint(1, 100)\n",
    "    learning_rate = reciprocal(0.0001, 0.1).rvs(random_state=search_iteration)\n",
    "    \n",
    "    n_inputs = 2 + 4\n",
    "    logdir = log_dir(\"logreg\")\n",
    "    \n",
    "    print(\"Iteration: \", search_iteration)\n",
    "    print(\"   logdir: \", logdir)\n",
    "    print(\"   batch_size: \", batch_size)\n",
    "    print(\"   learning_rate: \", learning_rate)\n",
    "    print(\"   training: \", end=\"\")\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "    \n",
    "    y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(\n",
    "                        X, y, learning_rate=learning_rate)\n",
    "    \n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    \n",
    "    n_epochs = 10001\n",
    "    n_batches = int(np.ceil(m / batch_size))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_index in range(n_batches):\n",
    "                X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "            if epoch % 500 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "        \n",
    "        saver.save(sess, final_model_path)\n",
    "        \n",
    "        print()\n",
    "        y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        y_pred = (y_proba_val >= 0.5)\n",
    "        \n",
    "        print(\"   precision: \", precision_score(y_test, y_pred))\n",
    "        print(\"   recall: \", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reciprocal()` function from SciPy's `stats` module returns a random distribution that is commonly used when you have no idea of the optimal scale of a hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
